{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1742d6",
      "metadata": {
        "id": "0f1742d6"
      },
      "outputs": [],
      "source": [
        "# models/mlp_fusion.py (Optimized for Google Colab Pro+ GPU)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe94f09f",
      "metadata": {
        "id": "fe94f09f"
      },
      "outputs": [],
      "source": [
        "# Speed tweaks for Colab\n",
        "torch.backends.cudnn.benchmark = True\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b7f58c",
      "metadata": {
        "id": "02b7f58c"
      },
      "outputs": [],
      "source": [
        "KEY_PATH = \"Key.json\"\n",
        "fs = gcsfs.GCSFileSystem(token=KEY_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b609d9",
      "metadata": {
        "id": "a4b609d9"
      },
      "outputs": [],
      "source": [
        "# Define features\n",
        "eeg_features = [\"Mobility\", \"Complexity\", \"Spectral_Entropy\"]\n",
        "eye_features = [\n",
        "    \"mean_pupil_size\", \"std_pupil_size\",\n",
        "    \"mean_latency\", \"std_latency\",\n",
        "    \"mean_gaze_vector\", \"std_gaze_vector\"\n",
        "]\n",
        "beh_features = [\"mean_rt\", \"accuracy\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201c4e03",
      "metadata": {
        "id": "201c4e03"
      },
      "outputs": [],
      "source": [
        "# Data preparation pipeline\n",
        "def extract_patient_id(name):\n",
        "    match = re.findall(r'(A\\d{5,})', str(name))\n",
        "    return match[0] if match else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb99438",
      "metadata": {
        "id": "feb99438"
      },
      "outputs": [],
      "source": [
        "def load_csv(fs, path, id_col):\n",
        "    with fs.open(path, 'r') as f:\n",
        "        df = pd.read_csv(f)\n",
        "    df[\"patient_id\"] = df[id_col].apply(extract_patient_id)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d1520a",
      "metadata": {
        "id": "e5d1520a"
      },
      "outputs": [],
      "source": [
        "def group_features(df, features):\n",
        "    return df.groupby(\"patient_id\")[features].mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f28c13",
      "metadata": {
        "id": "96f28c13"
      },
      "outputs": [],
      "source": [
        "def impute_missing_rows(modality_df, features, missing_ids):\n",
        "    avg_values = modality_df[features].mean()\n",
        "    imputed = pd.DataFrame([{**{\"patient_id\": pid}, **avg_values.to_dict()} for pid in missing_ids])\n",
        "    return pd.concat([modality_df, imputed], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb81a42f",
      "metadata": {
        "id": "fb81a42f",
        "outputId": "86a212ad-2949-4839-f728-477c0c3e18d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_60074/1353517648.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(f)\n"
          ]
        }
      ],
      "source": [
        "eeg_df = load_csv(fs, \"gs://eegchild/processed_features/merged_features.csv\", \"file_name\")\n",
        "eye_df = load_csv(fs, \"gs://eegchild/processed_asd_features.csv\", \"file_name\")\n",
        "beh_df = load_csv(fs, \"gs://eegchild/processed_features/behavioral_features.csv\", \"file\")\n",
        "label_df = load_csv(fs, \"gs://eegchild/MIPDB_PublicFile.csv\", \"ID\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a277297",
      "metadata": {
        "id": "7a277297"
      },
      "outputs": [],
      "source": [
        "label_df = label_df.rename(columns={\"DX_Status\": \"diagnosis_status\"})\n",
        "label_df[\"diagnosis_status\"] = label_df[\"diagnosis_status\"].replace({2: 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c870a9",
      "metadata": {
        "id": "d1c870a9"
      },
      "outputs": [],
      "source": [
        "for df in [eeg_df, eye_df, beh_df, label_df]:\n",
        "    df[\"patient_id\"] = df[\"patient_id\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b6974f3",
      "metadata": {
        "id": "3b6974f3"
      },
      "outputs": [],
      "source": [
        "grouped_eeg = group_features(eeg_df, eeg_features)\n",
        "grouped_eye = group_features(eye_df, eye_features)\n",
        "grouped_beh = group_features(beh_df, beh_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa77d408",
      "metadata": {
        "id": "fa77d408"
      },
      "outputs": [],
      "source": [
        "expected_ids = set(label_df[\"patient_id\"])\n",
        "grouped_eeg = impute_missing_rows(grouped_eeg, eeg_features, expected_ids - set(grouped_eeg[\"patient_id\"]))\n",
        "grouped_eye = impute_missing_rows(grouped_eye, eye_features, expected_ids - set(grouped_eye[\"patient_id\"]))\n",
        "grouped_beh = impute_missing_rows(grouped_beh, beh_features, expected_ids - set(grouped_beh[\"patient_id\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ef6a62",
      "metadata": {
        "id": "f1ef6a62"
      },
      "outputs": [],
      "source": [
        "eeg_merged = grouped_eeg.merge(label_df, on=\"patient_id\")\n",
        "eye_merged = grouped_eye.merge(label_df, on=\"patient_id\")\n",
        "beh_merged = grouped_beh.merge(label_df, on=\"patient_id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5cb12b",
      "metadata": {
        "id": "5e5cb12b"
      },
      "outputs": [],
      "source": [
        "common_ids = set(eeg_merged[\"patient_id\"]) & set(eye_merged[\"patient_id\"]) & set(beh_merged[\"patient_id\"])\n",
        "eeg_final = eeg_merged[eeg_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "eye_final = eye_merged[eye_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "beh_final = beh_merged[beh_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22197e6",
      "metadata": {
        "id": "f22197e6"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.DataFrame({\"patient_id\": eeg_final[\"patient_id\"], \"label\": eeg_final[\"diagnosis_status\"]})\n",
        "for feat in eeg_features:\n",
        "    merged_df[f\"eeg_{feat}\"] = eeg_final[feat]\n",
        "for feat in eye_features:\n",
        "    merged_df[f\"eye_{feat}\"] = eye_final[feat]\n",
        "for feat in beh_features:\n",
        "    merged_df[f\"beh_{feat}\"] = beh_final[feat]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1adaf65",
      "metadata": {
        "id": "c1adaf65"
      },
      "outputs": [],
      "source": [
        "class_0 = merged_df[merged_df[\"label\"] == 0]\n",
        "class_1 = merged_df[merged_df[\"label\"] == 1]\n",
        "balanced_df = pd.concat([\n",
        "    class_0.sample(n=63, replace=True, random_state=42),\n",
        "    class_1.sample(n=63, replace=True, random_state=42)\n",
        "]).sample(frac=1, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292285b8-010d-4c0c-827e-6091b1a80f5f",
      "metadata": {
        "id": "292285b8-010d-4c0c-827e-6091b1a80f5f",
        "outputId": "7c7eba2e-1080-4cc8-9b95-90b6632d50fa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient_id</th>\n",
              "      <th>label</th>\n",
              "      <th>eeg_Mobility</th>\n",
              "      <th>eeg_Complexity</th>\n",
              "      <th>eeg_Spectral_Entropy</th>\n",
              "      <th>eye_mean_pupil_size</th>\n",
              "      <th>eye_std_pupil_size</th>\n",
              "      <th>eye_mean_latency</th>\n",
              "      <th>eye_std_latency</th>\n",
              "      <th>eye_mean_gaze_vector</th>\n",
              "      <th>eye_std_gaze_vector</th>\n",
              "      <th>beh_mean_rt</th>\n",
              "      <th>beh_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00054215</td>\n",
              "      <td>1</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>13.826831</td>\n",
              "      <td>4.052553</td>\n",
              "      <td>2583.161045</td>\n",
              "      <td>2725.608537</td>\n",
              "      <td>-0.100949</td>\n",
              "      <td>0.245738</td>\n",
              "      <td>86.500000</td>\n",
              "      <td>36.979167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00055623</td>\n",
              "      <td>0</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>15.103591</td>\n",
              "      <td>4.893661</td>\n",
              "      <td>1999.052234</td>\n",
              "      <td>2871.989252</td>\n",
              "      <td>-0.155778</td>\n",
              "      <td>0.204640</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>46.205128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00056428</td>\n",
              "      <td>1</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>12.509149</td>\n",
              "      <td>5.124643</td>\n",
              "      <td>2221.530068</td>\n",
              "      <td>4376.657860</td>\n",
              "      <td>-0.132613</td>\n",
              "      <td>0.205558</td>\n",
              "      <td>86.968468</td>\n",
              "      <td>46.046014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00052593</td>\n",
              "      <td>1</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>12.023346</td>\n",
              "      <td>5.022370</td>\n",
              "      <td>2381.746719</td>\n",
              "      <td>4682.851008</td>\n",
              "      <td>-0.150730</td>\n",
              "      <td>0.202051</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>40.871795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00056762</td>\n",
              "      <td>1</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>11.257631</td>\n",
              "      <td>4.154228</td>\n",
              "      <td>2026.131148</td>\n",
              "      <td>2789.267988</td>\n",
              "      <td>-0.143232</td>\n",
              "      <td>0.198087</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>33.212761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>A00054917</td>\n",
              "      <td>1</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>18.102869</td>\n",
              "      <td>5.497564</td>\n",
              "      <td>2262.257843</td>\n",
              "      <td>3805.833094</td>\n",
              "      <td>-0.103817</td>\n",
              "      <td>0.285277</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>47.832858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>A00056990</td>\n",
              "      <td>0</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>16.659047</td>\n",
              "      <td>4.396053</td>\n",
              "      <td>1959.216453</td>\n",
              "      <td>1555.443846</td>\n",
              "      <td>-0.198504</td>\n",
              "      <td>0.127673</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>41.580247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>A00063558</td>\n",
              "      <td>1</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>12.509149</td>\n",
              "      <td>5.124643</td>\n",
              "      <td>2221.530068</td>\n",
              "      <td>4376.657860</td>\n",
              "      <td>-0.132613</td>\n",
              "      <td>0.205558</td>\n",
              "      <td>86.968468</td>\n",
              "      <td>46.046014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>A00062408</td>\n",
              "      <td>0</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>12.427230</td>\n",
              "      <td>4.897684</td>\n",
              "      <td>2104.032104</td>\n",
              "      <td>5287.226366</td>\n",
              "      <td>-0.081447</td>\n",
              "      <td>0.240537</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>54.617284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>A00063558</td>\n",
              "      <td>1</td>\n",
              "      <td>0.192332</td>\n",
              "      <td>2.461543</td>\n",
              "      <td>2.219917</td>\n",
              "      <td>12.509149</td>\n",
              "      <td>5.124643</td>\n",
              "      <td>2221.530068</td>\n",
              "      <td>4376.657860</td>\n",
              "      <td>-0.132613</td>\n",
              "      <td>0.205558</td>\n",
              "      <td>86.968468</td>\n",
              "      <td>46.046014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>126 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    patient_id  label  eeg_Mobility  eeg_Complexity  eeg_Spectral_Entropy  \\\n",
              "0    A00054215      1      0.192332        2.461543              2.219917   \n",
              "1    A00055623      0      0.192332        2.461543              2.219917   \n",
              "2    A00056428      1      0.192332        2.461543              2.219917   \n",
              "3    A00052593      1      0.192332        2.461543              2.219917   \n",
              "4    A00056762      1      0.192332        2.461543              2.219917   \n",
              "..         ...    ...           ...             ...                   ...   \n",
              "121  A00054917      1      0.192332        2.461543              2.219917   \n",
              "122  A00056990      0      0.192332        2.461543              2.219917   \n",
              "123  A00063558      1      0.192332        2.461543              2.219917   \n",
              "124  A00062408      0      0.192332        2.461543              2.219917   \n",
              "125  A00063558      1      0.192332        2.461543              2.219917   \n",
              "\n",
              "     eye_mean_pupil_size  eye_std_pupil_size  eye_mean_latency  \\\n",
              "0              13.826831            4.052553       2583.161045   \n",
              "1              15.103591            4.893661       1999.052234   \n",
              "2              12.509149            5.124643       2221.530068   \n",
              "3              12.023346            5.022370       2381.746719   \n",
              "4              11.257631            4.154228       2026.131148   \n",
              "..                   ...                 ...               ...   \n",
              "121            18.102869            5.497564       2262.257843   \n",
              "122            16.659047            4.396053       1959.216453   \n",
              "123            12.509149            5.124643       2221.530068   \n",
              "124            12.427230            4.897684       2104.032104   \n",
              "125            12.509149            5.124643       2221.530068   \n",
              "\n",
              "     eye_std_latency  eye_mean_gaze_vector  eye_std_gaze_vector  beh_mean_rt  \\\n",
              "0        2725.608537             -0.100949             0.245738    86.500000   \n",
              "1        2871.989252             -0.155778             0.204640    87.000000   \n",
              "2        4376.657860             -0.132613             0.205558    86.968468   \n",
              "3        4682.851008             -0.150730             0.202051    87.000000   \n",
              "4        2789.267988             -0.143232             0.198087    87.000000   \n",
              "..               ...                   ...                  ...          ...   \n",
              "121      3805.833094             -0.103817             0.285277    87.000000   \n",
              "122      1555.443846             -0.198504             0.127673    87.000000   \n",
              "123      4376.657860             -0.132613             0.205558    86.968468   \n",
              "124      5287.226366             -0.081447             0.240537    87.000000   \n",
              "125      4376.657860             -0.132613             0.205558    86.968468   \n",
              "\n",
              "     beh_accuracy  \n",
              "0       36.979167  \n",
              "1       46.205128  \n",
              "2       46.046014  \n",
              "3       40.871795  \n",
              "4       33.212761  \n",
              "..            ...  \n",
              "121     47.832858  \n",
              "122     41.580247  \n",
              "123     46.046014  \n",
              "124     54.617284  \n",
              "125     46.046014  \n",
              "\n",
              "[126 rows x 13 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e4da2e-8611-4ae0-bb2e-5e78a0d942f7",
      "metadata": {
        "id": "22e4da2e-8611-4ae0-bb2e-5e78a0d942f7"
      },
      "outputs": [],
      "source": [
        "all_feature_cols = [c for c in balanced_df.columns if c.startswith(\"eeg_\") or c.startswith(\"eye_\") or c.startswith(\"beh_\")]\n",
        "for col in all_feature_cols:\n",
        "    balanced_df[col] = pd.to_numeric(balanced_df[col], errors='coerce').fillna(0).astype(np.float32)\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.eeg_cols = [c for c in self.df.columns if c.startswith(\"eeg_\")]\n",
        "        self.eye_cols = [c for c in self.df.columns if c.startswith(\"eye_\")]\n",
        "        self.beh_cols = [c for c in self.df.columns if c.startswith(\"beh_\")]\n",
        "        for c in self.eeg_cols + self.eye_cols + self.beh_cols:\n",
        "            self.df[c] = pd.to_numeric(self.df[c], errors=\"coerce\").fillna(0).astype(np.float32)\n",
        "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        eeg = torch.tensor(row[self.eeg_cols].values.astype(np.float32), dtype=torch.float32)\n",
        "        eye = torch.tensor(row[self.eye_cols].values.astype(np.float32), dtype=torch.float32)\n",
        "        beh = torch.tensor(row[self.beh_cols].values.astype(np.float32), dtype=torch.float32)\n",
        "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
        "        return eeg, eye, beh, label\n",
        "\n",
        "def create_loaders(df, train_ratio=0.7, val_ratio=0.15, batch_size=16):\n",
        "    train_df, temp_df = train_test_split(df, test_size=1 - train_ratio, stratify=df[\"label\"], random_state=42)\n",
        "    val_size = val_ratio / (1 - train_ratio)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=1 - val_size, stratify=temp_df[\"label\"], random_state=42)\n",
        "\n",
        "    train_ds = MultimodalDataset(train_df)\n",
        "    val_ds = MultimodalDataset(val_df)\n",
        "    test_ds = MultimodalDataset(test_df)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# [model + training + evaluation remains unchanged]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77c3fe7-c03f-4e8d-ab5f-75b69c7fdf36",
      "metadata": {
        "id": "c77c3fe7-c03f-4e8d-ab5f-75b69c7fdf36"
      },
      "outputs": [],
      "source": [
        "# Add MLP model + training + evaluation (continued from previous)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature normalization before DataLoader creation\n",
        "scaler = StandardScaler()\n",
        "balanced_df[all_feature_cols] = scaler.fit_transform(balanced_df[all_feature_cols])\n",
        "\n",
        "# Define weighted loss based on label distribution\n",
        "label_counts = balanced_df['label'].value_counts().to_dict()\n",
        "total = sum(label_counts.values())\n",
        "weights = [total / label_counts.get(i, 1) for i in range(2)]\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d051fb78-59a9-44ff-bf88-2668f71f42e9",
      "metadata": {
        "id": "d051fb78-59a9-44ff-bf88-2668f71f42e9",
        "outputId": "6ea092bf-2b48-4a02-ac11-86478833141a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient_id</th>\n",
              "      <th>label</th>\n",
              "      <th>eeg_Mobility</th>\n",
              "      <th>eeg_Complexity</th>\n",
              "      <th>eeg_Spectral_Entropy</th>\n",
              "      <th>eye_mean_pupil_size</th>\n",
              "      <th>eye_std_pupil_size</th>\n",
              "      <th>eye_mean_latency</th>\n",
              "      <th>eye_std_latency</th>\n",
              "      <th>eye_mean_gaze_vector</th>\n",
              "      <th>eye_std_gaze_vector</th>\n",
              "      <th>beh_mean_rt</th>\n",
              "      <th>beh_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00054215</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>0.449923</td>\n",
              "      <td>-1.008182</td>\n",
              "      <td>1.304757</td>\n",
              "      <td>-0.907991</td>\n",
              "      <td>0.791119</td>\n",
              "      <td>0.680423</td>\n",
              "      <td>-2.731109</td>\n",
              "      <td>-0.940669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00055623</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>0.972018</td>\n",
              "      <td>-0.323739</td>\n",
              "      <td>-0.865559</td>\n",
              "      <td>-0.834152</td>\n",
              "      <td>-0.708414</td>\n",
              "      <td>-0.330877</td>\n",
              "      <td>0.243759</td>\n",
              "      <td>0.036823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00056428</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>-0.088906</td>\n",
              "      <td>-0.135780</td>\n",
              "      <td>-0.038921</td>\n",
              "      <td>-0.075152</td>\n",
              "      <td>-0.074858</td>\n",
              "      <td>-0.308273</td>\n",
              "      <td>0.056150</td>\n",
              "      <td>0.019965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00052593</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>-0.287562</td>\n",
              "      <td>-0.219003</td>\n",
              "      <td>0.556381</td>\n",
              "      <td>0.079301</td>\n",
              "      <td>-0.570354</td>\n",
              "      <td>-0.394587</td>\n",
              "      <td>0.243759</td>\n",
              "      <td>-0.528244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00056762</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>-0.600680</td>\n",
              "      <td>-0.925445</td>\n",
              "      <td>-0.764945</td>\n",
              "      <td>-0.875879</td>\n",
              "      <td>-0.365294</td>\n",
              "      <td>-0.492112</td>\n",
              "      <td>0.243759</td>\n",
              "      <td>-1.339720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>A00054917</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>2.198489</td>\n",
              "      <td>0.167681</td>\n",
              "      <td>0.112408</td>\n",
              "      <td>-0.363093</td>\n",
              "      <td>0.712696</td>\n",
              "      <td>1.653335</td>\n",
              "      <td>0.243759</td>\n",
              "      <td>0.209282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>A00056990</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>1.608079</td>\n",
              "      <td>-0.728662</td>\n",
              "      <td>-1.013574</td>\n",
              "      <td>-1.498257</td>\n",
              "      <td>-1.876939</td>\n",
              "      <td>-2.224771</td>\n",
              "      <td>0.243759</td>\n",
              "      <td>-0.453184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>A00063558</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>-0.088906</td>\n",
              "      <td>-0.135780</td>\n",
              "      <td>-0.038921</td>\n",
              "      <td>-0.075152</td>\n",
              "      <td>-0.074858</td>\n",
              "      <td>-0.308273</td>\n",
              "      <td>0.056150</td>\n",
              "      <td>0.019965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>A00062408</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>-0.122405</td>\n",
              "      <td>-0.320465</td>\n",
              "      <td>-0.475497</td>\n",
              "      <td>0.384166</td>\n",
              "      <td>1.324482</td>\n",
              "      <td>0.552425</td>\n",
              "      <td>0.243759</td>\n",
              "      <td>0.928093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>A00063558</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.11287</td>\n",
              "      <td>0.100533</td>\n",
              "      <td>-0.106785</td>\n",
              "      <td>-0.088906</td>\n",
              "      <td>-0.135780</td>\n",
              "      <td>-0.038921</td>\n",
              "      <td>-0.075152</td>\n",
              "      <td>-0.074858</td>\n",
              "      <td>-0.308273</td>\n",
              "      <td>0.056150</td>\n",
              "      <td>0.019965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>126 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    patient_id  label  eeg_Mobility  eeg_Complexity  eeg_Spectral_Entropy  \\\n",
              "0    A00054215      1      -0.11287        0.100533             -0.106785   \n",
              "1    A00055623      0      -0.11287        0.100533             -0.106785   \n",
              "2    A00056428      1      -0.11287        0.100533             -0.106785   \n",
              "3    A00052593      1      -0.11287        0.100533             -0.106785   \n",
              "4    A00056762      1      -0.11287        0.100533             -0.106785   \n",
              "..         ...    ...           ...             ...                   ...   \n",
              "121  A00054917      1      -0.11287        0.100533             -0.106785   \n",
              "122  A00056990      0      -0.11287        0.100533             -0.106785   \n",
              "123  A00063558      1      -0.11287        0.100533             -0.106785   \n",
              "124  A00062408      0      -0.11287        0.100533             -0.106785   \n",
              "125  A00063558      1      -0.11287        0.100533             -0.106785   \n",
              "\n",
              "     eye_mean_pupil_size  eye_std_pupil_size  eye_mean_latency  \\\n",
              "0               0.449923           -1.008182          1.304757   \n",
              "1               0.972018           -0.323739         -0.865559   \n",
              "2              -0.088906           -0.135780         -0.038921   \n",
              "3              -0.287562           -0.219003          0.556381   \n",
              "4              -0.600680           -0.925445         -0.764945   \n",
              "..                   ...                 ...               ...   \n",
              "121             2.198489            0.167681          0.112408   \n",
              "122             1.608079           -0.728662         -1.013574   \n",
              "123            -0.088906           -0.135780         -0.038921   \n",
              "124            -0.122405           -0.320465         -0.475497   \n",
              "125            -0.088906           -0.135780         -0.038921   \n",
              "\n",
              "     eye_std_latency  eye_mean_gaze_vector  eye_std_gaze_vector  beh_mean_rt  \\\n",
              "0          -0.907991              0.791119             0.680423    -2.731109   \n",
              "1          -0.834152             -0.708414            -0.330877     0.243759   \n",
              "2          -0.075152             -0.074858            -0.308273     0.056150   \n",
              "3           0.079301             -0.570354            -0.394587     0.243759   \n",
              "4          -0.875879             -0.365294            -0.492112     0.243759   \n",
              "..               ...                   ...                  ...          ...   \n",
              "121        -0.363093              0.712696             1.653335     0.243759   \n",
              "122        -1.498257             -1.876939            -2.224771     0.243759   \n",
              "123        -0.075152             -0.074858            -0.308273     0.056150   \n",
              "124         0.384166              1.324482             0.552425     0.243759   \n",
              "125        -0.075152             -0.074858            -0.308273     0.056150   \n",
              "\n",
              "     beh_accuracy  \n",
              "0       -0.940669  \n",
              "1        0.036823  \n",
              "2        0.019965  \n",
              "3       -0.528244  \n",
              "4       -1.339720  \n",
              "..            ...  \n",
              "121      0.209282  \n",
              "122     -0.453184  \n",
              "123      0.019965  \n",
              "124      0.928093  \n",
              "125      0.019965  \n",
              "\n",
              "[126 rows x 13 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "balanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872ca5c2-8914-407e-94fd-b258732a8cf9",
      "metadata": {
        "id": "872ca5c2-8914-407e-94fd-b258732a8cf9"
      },
      "outputs": [],
      "source": [
        "# Further Analysis for TransformerFusion and GATFusion\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyArrowPatch, Circle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# Self-Attention Heatmap Visualizer (Transformer)\n",
        "@torch.no_grad()\n",
        "def visualize_attention_weights(model, eeg, eye, beh):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    eeg = eeg.to(device).unsqueeze(0)\n",
        "    eye = eye.to(device).unsqueeze(0)\n",
        "    beh = beh.to(device).unsqueeze(0)\n",
        "\n",
        "    eeg_h = F.relu(model.eeg_fc(eeg))\n",
        "    eye_h = F.relu(model.eye_fc(eye))\n",
        "    beh_h = F.relu(model.beh_fc(beh))\n",
        "    x = torch.stack([eeg_h, eye_h, beh_h], dim=1)\n",
        "\n",
        "    attn_layer = model.transformer.layers[0].self_attn\n",
        "    attn_output, attn_weights = attn_layer(x, x, x, need_weights=True)\n",
        "    weights = attn_weights.squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "    for h in range(weights.shape[0]):\n",
        "        visualize_connectogram(weights[h], title=f\"Transformer Head {h+1} Connectogram\", save_path=f\"results/transformer_connectogram_{h+1}.png\")\n",
        "\n",
        "# GAT attention visualizer\n",
        "@torch.no_grad()\n",
        "def visualize_gat_attention(model, eeg, eye, beh):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    eeg = eeg.to(device).unsqueeze(0)\n",
        "    eye = eye.to(device).unsqueeze(0)\n",
        "    beh = beh.to(device).unsqueeze(0)\n",
        "\n",
        "    eeg_h = F.relu(model.eeg_fc(eeg))\n",
        "    eye_h = F.relu(model.eye_fc(eye))\n",
        "    beh_h = F.relu(model.beh_fc(beh))\n",
        "    x = torch.stack([eeg_h, eye_h, beh_h], dim=1)\n",
        "\n",
        "    h = model.gat.lin(x)\n",
        "    att_src = model.gat.att_src.view(1, 1, -1)\n",
        "    att_dst = model.gat.att_dst.view(1, 1, -1)\n",
        "\n",
        "    src = (h * att_src).sum(dim=-1)\n",
        "    dst = (h * att_dst).sum(dim=-1)\n",
        "    e = model.gat.leaky_relu(src.unsqueeze(2) + dst.unsqueeze(1))\n",
        "    alpha = F.softmax(e, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    visualize_connectogram(alpha, title=\"GAT Connectogram\", save_path=\"results/gat_connectogram.png\")\n",
        "\n",
        "# Connectogram-Style Visualization\n",
        "def visualize_connectogram(matrix, title=\"Connectogram\", save_path=\"connectogram.png\"):\n",
        "    matrix = np.asarray(matrix)\n",
        "    if matrix.ndim == 1:\n",
        "        matrix = np.expand_dims(matrix, axis=0)\n",
        "    if matrix.shape != (3, 3):\n",
        "        print(f\"Invalid shape for matrix: {matrix.shape}, skipping plot.\")\n",
        "        return\n",
        "\n",
        "    labels = [\"EEG\", \"Eye\", \"Beh\"]\n",
        "    pos = {\n",
        "        \"EEG\": (0, 1),\n",
        "        \"Eye\": (-1, -1),\n",
        "        \"Beh\": (1, -1)\n",
        "    }\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.set_xlim(-2, 2)\n",
        "    ax.set_ylim(-2, 2)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "\n",
        "    for label in labels:\n",
        "        x, y = pos[label]\n",
        "        circle = Circle((x, y), 0.15, color='skyblue', ec='black', lw=1.5)\n",
        "        ax.add_patch(circle)\n",
        "        ax.text(x, y, label, fontsize=12, ha='center', va='center', weight='bold')\n",
        "\n",
        "    for i, src in enumerate(labels):\n",
        "        for j, tgt in enumerate(labels):\n",
        "            if i != j:\n",
        "                x1, y1 = pos[src]\n",
        "                x2, y2 = pos[tgt]\n",
        "                if matrix.ndim == 2:\n",
        "                    weight = matrix[i][j]\n",
        "                    arrow = FancyArrowPatch((x1, y1), (x2, y2), connectionstyle=\"arc3,rad=0.2\",\n",
        "                                            arrowstyle='-|>', mutation_scale=15,\n",
        "                                            lw=weight * 5, color=plt.cm.viridis(weight))\n",
        "                    ax.add_patch(arrow)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Per-Modality Ablation Evaluation\n",
        "@torch.no_grad()\n",
        "def ablation_test(model_class, base_model, device):\n",
        "    test_results = {}\n",
        "\n",
        "    for combo in [\"eeg\", \"eye\", \"beh\", \"eeg+eye\", \"eeg+beh\", \"eye+beh\", \"all\"]:\n",
        "        model = model_class(len(eeg_features), len(eye_features), len(beh_features)).to(device)\n",
        "        model.load_state_dict(base_model.state_dict())\n",
        "        model.eval()\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        for eeg, eye, beh, labels in test_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "\n",
        "            if combo == \"all\":\n",
        "                pass\n",
        "            elif combo == \"eeg\":\n",
        "                eye = torch.zeros_like(eye)\n",
        "                beh = torch.zeros_like(beh)\n",
        "            elif combo == \"eye\":\n",
        "                eeg = torch.zeros_like(eeg)\n",
        "                beh = torch.zeros_like(beh)\n",
        "            elif combo == \"beh\":\n",
        "                eeg = torch.zeros_like(eeg)\n",
        "                eye = torch.zeros_like(eye)\n",
        "            elif combo == \"eeg+eye\":\n",
        "                beh = torch.zeros_like(beh)\n",
        "            elif combo == \"eeg+beh\":\n",
        "                eye = torch.zeros_like(eye)\n",
        "            elif combo == \"eye+beh\":\n",
        "                eeg = torch.zeros_like(eeg)\n",
        "\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "        report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "        test_results[combo] = {\"acc\": acc, \"f1\": report[\"macro avg\"][\"f1-score\"]}\n",
        "\n",
        "    return test_results\n",
        "\n",
        "# Additional visualizations: ROC and PR Curve\n",
        "@torch.no_grad()\n",
        "def plot_roc_pr_curves(model, loader, device, title_prefix=\"model\"):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    for eeg, eye, beh, labels in loader:\n",
        "        eeg, eye, beh = eeg.to(device), eye.to(device), beh.to(device)\n",
        "        outputs = model(eeg, eye, beh)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "    prec, rec, _ = precision_recall_curve(all_labels, all_probs)\n",
        "\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    pr_auc = auc(rec, prec)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{title_prefix} ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(f\"results/{title_prefix.lower()}_roc.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(rec, prec, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'{title_prefix} Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.savefig(f\"results/{title_prefix.lower()}_pr.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    eeg, eye, beh, label = next(iter(test_loader))\n",
        "    visualize_attention_weights(model_t, eeg[0], eye[0], beh[0])\n",
        "    ablation_scores = ablation_test(TransformerFusion, model_t, device)\n",
        "    print(\"\\n▶ Transformer Ablation Study Results:\")\n",
        "    for k, v in ablation_scores.items():\n",
        "        print(f\"{k.upper():9} — Accuracy: {v['acc']:.2f}, F1: {v['f1']:.2f}\")\n",
        "\n",
        "    if 'model_gat' in globals():\n",
        "        visualize_gat_attention(model_gat, eeg[0], eye[0], beh[0])\n",
        "        plot_roc_pr_curves(model_gat, test_loader, device, title_prefix=\"GAT\")\n",
        "\n",
        "    plot_roc_pr_curves(model_t, test_loader, device, title_prefix=\"Transformer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fcb2ac-3f4f-4c4b-b41a-9d045e512e21",
      "metadata": {
        "id": "f8fcb2ac-3f4f-4c4b-b41a-9d045e512e21"
      },
      "outputs": [],
      "source": [
        "#ImprovedTransformerFusion (after TransformerFusion block)\n",
        "# Improved version of TransformerFusion with deeper encoding and attention fusion\n",
        "class ModalityEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, model_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(model_dim, model_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class AttentionFusion(nn.Module):\n",
        "    def __init__(self, model_dim):\n",
        "        super().__init__()\n",
        "        self.attn_weights = nn.Linear(model_dim, 1)\n",
        "\n",
        "    def forward(self, xs):  # xs is a list of modality embeddings [B, D]\n",
        "        x = torch.stack(xs, dim=1)  # Shape: (B, M, D)\n",
        "        scores = self.attn_weights(x).squeeze(-1)  # Shape: (B, M)\n",
        "        w = F.softmax(scores, dim=1).unsqueeze(-1)  # Shape: (B, M, 1)\n",
        "        return (x * w).sum(dim=1)  # Fused output shape: (B, D)\n",
        "\n",
        "class ImprovedTransformerFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, model_dim=64):\n",
        "        super().__init__()\n",
        "        self.eeg_encoder = ModalityEncoder(eeg_dim, model_dim)\n",
        "        self.eye_encoder = ModalityEncoder(eye_dim, model_dim)\n",
        "        self.beh_encoder = ModalityEncoder(beh_dim, model_dim)\n",
        "\n",
        "        self.fusion = AttentionFusion(model_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(model_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_h = self.eeg_encoder(eeg)\n",
        "        eye_h = self.eye_encoder(eye)\n",
        "        beh_h = self.beh_encoder(beh)\n",
        "        fused = self.fusion([eeg_h, eye_h, beh_h])\n",
        "        return self.classifier(fused)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b7e4e9-0e51-4942-8ae6-c54fbf97f885",
      "metadata": {
        "id": "06b7e4e9-0e51-4942-8ae6-c54fbf97f885"
      },
      "outputs": [],
      "source": [
        "model_improved = ImprovedTransformerFusion(\n",
        "    eeg_dim=len(eeg_features),\n",
        "    eye_dim=len(eye_features),\n",
        "    beh_dim=len(beh_features)\n",
        ").to(device)\n",
        "\n",
        "train_model(model_improved, train_loader, val_loader, device)\n",
        "model_improved.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n",
        "evaluate_model(model_improved, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f554e88-f226-46c6-aab2-7d152966e9dc",
      "metadata": {
        "id": "6f554e88-f226-46c6-aab2-7d152966e9dc"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "[NEW] Modality Embedding Layer — to embed modality-type tokens, improving cross-modal attention.\n",
        "\n",
        "[NEW] Learnable Positional Embeddings — optional but may benefit stability with small batch sizes.\n",
        "\n",
        "[MODIFIED] Early Layer Normalization — TransformerEncoderLayer in PyTorch uses pre-LN style.\n",
        "\n",
        "[MODIFIED] Deeper Classification Head — more expressive capacity, with regularization.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61f3f41-bf2c-4d69-a07e-9425f55d05c0",
      "metadata": {
        "id": "a61f3f41-bf2c-4d69-a07e-9425f55d05c0"
      },
      "outputs": [],
      "source": [
        "class TransformerFusionBlock(nn.Module):\n",
        "    def __init__(self, model_dim=32, nhead=4):\n",
        "        super().__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=model_dim, nhead=nhead, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
        "        self.attn_weights = nn.Parameter(torch.ones(3))\n",
        "\n",
        "    def forward(self, modalities):\n",
        "        x = torch.stack(modalities, dim=1)\n",
        "        x = self.transformer(x)\n",
        "        weights = torch.softmax(self.attn_weights, dim=0)\n",
        "        return (x * weights.view(1, -1, 1)).sum(dim=1)\n",
        "\n",
        "class ImprovedTransformerFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, model_dim=32):\n",
        "        super().__init__()\n",
        "        self.eeg_encoder = ModalityEncoder(eeg_dim, model_dim)\n",
        "        self.eye_encoder = ModalityEncoder(eye_dim, model_dim)\n",
        "        self.beh_encoder = ModalityEncoder(beh_dim, model_dim)\n",
        "        self.fusion = TransformerFusionBlock(model_dim=model_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(model_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        x_eeg = self.eeg_encoder(eeg)\n",
        "        x_eye = self.eye_encoder(eye)\n",
        "        x_beh = self.beh_encoder(beh)\n",
        "        fused = self.fusion([x_eeg, x_eye, x_beh])\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# Instantiate the improved model\n",
        "model = ImprovedTransformerFusion(\n",
        "    eeg_dim=len(EEG_FEATURES),\n",
        "    eye_dim=len(EYE_FEATURES),\n",
        "    beh_dim=len(BEHAV_FEATURES)\n",
        ").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc259a0-34d4-4ff2-8a86-7c2081aca609",
      "metadata": {
        "id": "1dc259a0-34d4-4ff2-8a86-7c2081aca609"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = BCEWithLogitsLoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7761b118-eaa9-43c7-a218-b598dd0c2f80",
      "metadata": {
        "id": "7761b118-eaa9-43c7-a218-b598dd0c2f80"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, correct, count = 0, 0, 0\n",
        "    all_labels, all_probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for eeg, eye, beh, labels in dataloader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device).unsqueeze(1).float()\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).int()\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            correct += (preds == labels.int()).sum().item()\n",
        "            count += labels.size(0)\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_probs.append(probs.cpu())\n",
        "    return total_loss / count, correct / count, torch.cat(all_labels), torch.cat(all_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42de5358-63c2-408b-9ebf-724a6711da21",
      "metadata": {
        "id": "42de5358-63c2-408b-9ebf-724a6711da21"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct, count = 0, 0, 0\n",
        "    for eeg, eye, beh, labels in dataloader:\n",
        "        eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device).unsqueeze(1).float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(eeg, eye, beh)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = (torch.sigmoid(outputs) >= 0.5).int()\n",
        "        correct += (preds == labels.int()).sum().item()\n",
        "        count += labels.size(0)\n",
        "    return total_loss / count, correct / count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9f0cb7-4c2c-4d52-a760-6f0654a12313",
      "metadata": {
        "id": "6a9f0cb7-4c2c-4d52-a760-6f0654a12313"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import copy\n",
        "\n",
        "best_acc = 0\n",
        "best_model = None\n",
        "patience, counter = 5, 0\n",
        "\n",
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc, val_labels, val_probs = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be569685-4c62-4448-897c-e940026faea0",
      "metadata": {
        "id": "be569685-4c62-4448-897c-e940026faea0"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(best_model)\n",
        "test_loss, test_acc, test_labels, test_probs = evaluate(model, test_loader, criterion)\n",
        "test_preds = (test_probs >= 0.5).int()\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0eec63-8b69-4909-8aef-970012911098",
      "metadata": {
        "id": "3c0eec63-8b69-4909-8aef-970012911098"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Updated NeuroXplain Pipeline - Further Improved Transformer Fusion Model with Dynamic Layer Depth, Weighted Sampling, Adaptive Optimizer,\n",
        "Label Smoothing, Bias Mitigation (Pre-, In-, and Post-Processing), and Responsible AI Additions\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "# Speed tweaks\n",
        "torch.backends.cudnn.benchmark = True\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Auth\n",
        "KEY_PATH = \"Key.json\"\n",
        "fs = gcsfs.GCSFileSystem(token=KEY_PATH)\n",
        "\n",
        "# Features\n",
        "EEG_FEATURES = [\"Mobility\", \"Complexity\", \"Spectral_Entropy\"]\n",
        "EYE_FEATURES = [\n",
        "    \"mean_pupil_size\", \"std_pupil_size\",\n",
        "    \"mean_latency\", \"std_latency\",\n",
        "    \"mean_gaze_vector\", \"std_gaze_vector\"\n",
        "]\n",
        "BEHAV_FEATURES = [\"mean_rt\", \"accuracy\"]\n",
        "\n",
        "# ----------------------------\n",
        "# ?? Transformer Model Block\n",
        "# ----------------------------\n",
        "class ModalityEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerFusionBlock(nn.Module):\n",
        "    def __init__(self, model_dim=32, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=model_dim, nhead=nhead, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.attn_weights = nn.Parameter(torch.ones(3))\n",
        "\n",
        "    def forward(self, modalities):\n",
        "        x = torch.stack(modalities, dim=1)\n",
        "        x = self.transformer(x)\n",
        "        weights = torch.softmax(self.attn_weights, dim=0)\n",
        "        return (x * weights.view(1, -1, 1)).sum(dim=1)\n",
        "\n",
        "class ImprovedTransformerFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, model_dim=32):\n",
        "        super().__init__()\n",
        "        self.eeg_encoder = ModalityEncoder(eeg_dim, model_dim)\n",
        "        self.eye_encoder = ModalityEncoder(eye_dim, model_dim)\n",
        "        self.beh_encoder = ModalityEncoder(beh_dim, model_dim)\n",
        "        self.fusion = TransformerFusionBlock(model_dim=model_dim, num_layers=2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(model_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        x_eeg = self.eeg_encoder(eeg)\n",
        "        x_eye = self.eye_encoder(eye)\n",
        "        x_beh = self.beh_encoder(beh)\n",
        "        fused = self.fusion([x_eeg, x_eye, x_beh])\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# ----------------------------\n",
        "# ? Updated Focal Loss with Label Smoothing\n",
        "# ----------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, smoothing=0.1, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.smoothing = smoothing\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = targets * (1.0 - self.smoothing) + 0.5 * self.smoothing\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
        "\n",
        "# ----------------------------\n",
        "# ?? Training with Bias Mitigation and RAI Additions\n",
        "# ----------------------------\n",
        "def train_model(model, train_loader, val_loader, patience=6, epochs=50):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = FocalLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    best_val_acc = 0\n",
        "    best_model = None\n",
        "    patience_counter = 0\n",
        "    train_losses, val_accuracies, all_preds, all_labels = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for eeg, eye, beh, labels in train_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.float().to(device).unsqueeze(1)\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pred = torch.sigmoid(outputs).detach().cpu().numpy() >= 0.5\n",
        "            correct += (pred == labels.cpu().numpy()).sum()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        train_losses.append(total_loss / len(train_loader))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for eeg, eye, beh, labels in val_loader:\n",
        "                eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.float().to(device).unsqueeze(1)\n",
        "                outputs = model(eeg, eye, beh)\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                preds = probs.cpu().numpy() >= 0.5\n",
        "                all_preds.extend(probs.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                correct += (preds == labels.cpu().numpy()).sum()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "        val_accuracies.append(val_acc)\n",
        "        print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "\n",
        "    # PR Curve\n",
        "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    plt.plot(recall, precision, marker='.')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"results/precision_recall_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Calibration Curve (Post-Processing Bias Check)\n",
        "    prob_true, prob_pred = calibration_curve(np.array(all_labels).flatten(), np.array(all_preds).flatten(), n_bins=10)\n",
        "    plt.plot(prob_pred, prob_true, marker='o')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.title('Calibration Curve')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('True Probability')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"results/calibration_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7090fbdf-437a-415d-8764-9389e1ca640a",
      "metadata": {
        "id": "7090fbdf-437a-415d-8764-9389e1ca640a"
      },
      "outputs": [],
      "source": [
        "!pip install fairlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "508d74f8-9ea4-4cd4-89b2-45deeb9c4fbc",
      "metadata": {
        "id": "508d74f8-9ea4-4cd4-89b2-45deeb9c4fbc"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Updated NeuroXplain Pipeline - Final Evaluation + SHAP Fix + Bias Mitigation\n",
        "\"\"\"\n",
        "\n",
        "# (Existing imports remain unchanged)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "from fairlearn.postprocessing import ThresholdOptimizer\n",
        "\n",
        "# (Model, loss, and training definitions remain unchanged)\n",
        "\n",
        "# ----------------------------\n",
        "# ? Evaluation Utility\n",
        "# ----------------------------\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    criterion = FocalLoss()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for eeg, eye, beh, labels in loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.float().to(device).unsqueeze(1)\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).int()\n",
        "            total_loss += loss.item()\n",
        "            correct += (preds == labels.int()).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(probs.detach().cpu().numpy())\n",
        "\n",
        "    acc = correct / total\n",
        "    preds_final = (np.array(all_preds) >= 0.5).astype(int)\n",
        "    print(classification_report(np.array(all_labels), preds_final, digits=4))\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    plt.plot(recall, precision, marker='.')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"results/precision_recall_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Calibration Curve\n",
        "    prob_true, prob_pred = calibration_curve(np.array(all_labels).flatten(), np.array(all_preds).flatten(), n_bins=10)\n",
        "    plt.plot(prob_pred, prob_true, marker='o')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.title('Calibration Curve')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('True Probability')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"results/calibration_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save predictions\n",
        "    pd.DataFrame({\"label\": all_labels, \"pred_prob\": np.array(all_preds).flatten(), \"pred\": preds_final.flatten()}).to_csv(\"results/test_predictions.csv\", index=False)\n",
        "\n",
        "    return acc, all_labels, all_preds\n",
        "\n",
        "# ----------------------------\n",
        "# ?? SHAP Explainability with Fix (detach numpy)\n",
        "# ----------------------------\n",
        "def explain_with_shap(model, loader):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    eeg_sample, eye_sample, beh_sample, _ = next(iter(loader))\n",
        "    eeg_sample = eeg_sample[:50].to(device)\n",
        "    eye_sample = eye_sample[:50].to(device)\n",
        "    beh_sample = beh_sample[:50].to(device)\n",
        "\n",
        "    def model_forward(X):\n",
        "        eeg_len = eeg_sample.shape[1]\n",
        "        eye_len = eye_sample.shape[1]\n",
        "        beh_len = beh_sample.shape[1]\n",
        "        eeg = torch.tensor(X[:, :eeg_len]).float().to(device)\n",
        "        eye = torch.tensor(X[:, eeg_len:eeg_len+eye_len]).float().to(device)\n",
        "        beh = torch.tensor(X[:, -beh_len:]).float().to(device)\n",
        "        return torch.sigmoid(model(eeg, eye, beh)).detach().cpu().numpy()\n",
        "\n",
        "    combined_input = torch.cat([eeg_sample.cpu(), eye_sample.cpu(), beh_sample.cpu()], dim=1).numpy()\n",
        "    explainer = shap.KernelExplainer(model_forward, combined_input)\n",
        "    shap_values = explainer.shap_values(combined_input)\n",
        "\n",
        "    all_features = [f\"eeg_{f}\" for f in EEG_FEATURES] + [f\"eye_{f}\" for f in EYE_FEATURES] + [f\"beh_{f}\" for f in BEHAV_FEATURES]\n",
        "    try:\n",
        "        if isinstance(shap_values, list) and len(shap_values) == 1:\n",
        "            shap.summary_plot(shap_values[0], combined_input, feature_names=all_features)\n",
        "        else:\n",
        "            shap.summary_plot(shap_values, combined_input, feature_names=all_features)\n",
        "    except Exception as e:\n",
        "        print(\"? SHAP summary_plot failed:\", e)\n",
        "\n",
        "    np.save(\"results/shap_values.npy\", shap_values)\n",
        "    np.save(\"results/shap_input.npy\", combined_input)\n",
        "\n",
        "# ----------------------------\n",
        "# ?? Main Execution\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Prepare data\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImprovedTransformerFusion(\n",
        "        eeg_dim=len(EEG_FEATURES),\n",
        "        eye_dim=len(EYE_FEATURES),\n",
        "        beh_dim=len(BEHAV_FEATURES),\n",
        "        model_dim=32\n",
        "    ).to(device)\n",
        "\n",
        "    # Train model\n",
        "\n",
        "#  Use the fitted internal estimator from Fairlearn\n",
        "trained_model = fair_clf._trained_predictor\n",
        "\n",
        "# Safe sample (small to avoid memory issues)\n",
        "shap_sample = X_train[:50]\n",
        "\n",
        "# Build SHAP explainer\n",
        "explainer = shap.Explainer(trained_model.predict, shap_sample)\n",
        "\n",
        "# Compute SHAP values on test subset\n",
        "shap_values = explainer(X_test[:50])\n",
        "\n",
        "# Feature names (from earlier)\n",
        "feature_names = feature_cols\n",
        "\n",
        "# Plot SHAP summary (fixes dimension mismatch errors)\n",
        "shap.summary_plot(shap_values.values, X_test[:50], feature_names=feature_names)\n",
        "    # Evaluate\n",
        "    print(\"\\n?? Final Test Performance:\")\n",
        "    acc, labels, preds = evaluate_model(trained_model, test_loader, device)\n",
        "\n",
        "    # Fairness Post-Processing: Threshold Optimizer Placeholder\n",
        "    try:\n",
        "        y_true = np.array(labels).flatten()\n",
        "        y_probs = np.array(preds).flatten()\n",
        "        threshold_optimizer = ThresholdOptimizer(\n",
        "            estimator=None, prefit=False, constraints=\"equalized_odds\"\n",
        "        )\n",
        "        print(\"\\n??  Post-processing bias mitigation via ThresholdOptimizer could be added here if groups are defined.\")\n",
        "    except Exception as e:\n",
        "        print(\"ThresholdOptimizer skipped (setup issue):\", e)\n",
        "\n",
        "    # Explain\n",
        "    print(\"\\n?? Generating SHAP explainability report...\")\n",
        "    explain_with_shap(trained_model, test_loader)\n",
        "\n",
        "    # Preview predictions\n",
        "    df_preds = pd.read_csv(\"results/test_predictions.csv\")\n",
        "    print(\"\\n?? Sample Predictions:\")\n",
        "    print(df_preds.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4392c1cf-11f6-4120-935d-35c88de37ff9",
      "metadata": {
        "id": "4392c1cf-11f6-4120-935d-35c88de37ff9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Re-defining demographic info due to kernel reset\n",
        "group_data = {\n",
        "    \"6-9\": {\n",
        "        \"male\": [\n",
        "            \"A00053375\", \"A00053480\", \"A00054400\", \"A00054488\", \"A00054673\", \"A00054766\", \"A00054836\",\n",
        "            \"A00054917\", \"A00055436\", \"A00055904\", \"A00055497\", \"A00054569\", \"A00054666\", \"A00055473\",\n",
        "            \"A00056002\", \"A00056257\", \"A00056693\"\n",
        "        ],\n",
        "        \"female\": [\n",
        "            \"A00054432\", \"A00055424\", \"A00055397\", \"A00054597\", \"A00055526\", \"A00055628\", \"A00055547\"\n",
        "        ]\n",
        "    },\n",
        "    \"10-11\": {\n",
        "        \"male\": [\n",
        "            \"A00051826\", \"A00053460\", \"A00054517\", \"A00054592\", \"A00055392\", \"A00055429\", \"A00055613\",\n",
        "            \"A00055623\", \"A00055649\", \"A00056428\", \"A00058596\", \"A00055469\", \"A00055486\"\n",
        "        ],\n",
        "        \"female\": [\n",
        "            \"A00054535\", \"A00054748\", \"A00055293\", \"A00054287\", \"A00055038\", \"A00055510\", \"A00055616\"\n",
        "        ]\n",
        "    },\n",
        "    \"12-13\": {\n",
        "        \"male\": [\n",
        "            \"A00051886\", \"A00051955\", \"A00053398\", \"A00053440\", \"A00053909\", \"A00054239\", \"A00054417\",\n",
        "            \"A00055540\", \"A00057135\", \"A00054907\", \"A00055662\", \"A00055682\", \"A00055731\", \"A00055745\",\n",
        "            \"A00055753\", \"A00059063\"\n",
        "        ],\n",
        "        \"female\": [\n",
        "            \"A00055801\", \"A00057599\", \"A00054359\", \"A00054469\", \"A00054721\", \"A00054852\"\n",
        "        ]\n",
        "    },\n",
        "    \"14-17\": {\n",
        "        \"male\": [\n",
        "            \"A00054817\", \"A00055055\", \"A00055077\", \"A00056054\", \"A00056116\", \"A00055730\", \"A00055085\",\n",
        "            \"A00055978\", \"A00054647\", \"A00055482\", \"A00055502\", \"A00055486\", \"A00055853\", \"A00056733\",\n",
        "            \"A00054623\", \"A00055865\", \"A00056158\"\n",
        "        ],\n",
        "        \"female\": [\n",
        "            \"A00054369\", \"A00056723\", \"A00055103\", \"A00055837\", \"A00055593\", \"A00056166\", \"A00056990\"\n",
        "        ]\n",
        "    },\n",
        "    \"18-24\": {\n",
        "        \"male\": [\n",
        "            \"A00054930\", \"A00055065\", \"A00054039\", \"A00057092\", \"A00062919\", \"A00058775\", \"A00053990\",\n",
        "            \"A00054023\"\n",
        "        ],\n",
        "        \"female\": [\n",
        "            \"A00054387\", \"A00054207\", \"A00054122\", \"A00056640\", \"A00059083\", \"A00063051\", \"A00056604\", \"A00063117\"\n",
        "        ]\n",
        "    },\n",
        "    \"25-44\": {\n",
        "        \"male\": [\n",
        "            \"A00062219\", \"A00062408\", \"A00062125\", \"A00062578\", \"A00062704\", \"A00062029\", \"A00062435\",\n",
        "            \"A00062951\", \"A00063029\", \"A00062279\"\n",
        "        ],\n",
        "        \"female\": [\n",
        "            \"A00062842\", \"A00062453\", \"A00062329\", \"A00063558\", \"A00062165\", \"A00063377\", \"A00062055\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Flatten to dataframe\n",
        "records = []\n",
        "for age_group, genders in group_data.items():\n",
        "    for gender, ids in genders.items():\n",
        "        for pid in ids:\n",
        "            records.append({\n",
        "                \"patient_id\": pid,\n",
        "                \"age_group\": age_group,\n",
        "                \"gender\": gender\n",
        "            })\n",
        "\n",
        "df_demo = pd.DataFrame(records)\n",
        "df_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02af186-b819-49fc-9183-e53223eb08e1",
      "metadata": {
        "id": "a02af186-b819-49fc-9183-e53223eb08e1"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Updated NeuroXplain Pipeline - Final Evaluation + SHAP Fix + Bias Mitigation + Demographics Integration + Fairness Evaluation + Threshold Optimizer + Adversarial Debiasing\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, accuracy_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "from fairlearn.postprocessing import ThresholdOptimizer\n",
        "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        "\n",
        "# ----------------------------\n",
        "# ? Demographic Mapping\n",
        "# ----------------------------\n",
        "patient_demographics = {\n",
        "    \"A00053375\": (\"6-9\", \"M\"), \"A00053480\": (\"6-9\", \"M\"), \"A00054400\": (\"6-9\", \"M\"), \"A00054488\": (\"6-9\", \"M\"),\n",
        "    \"A00054673\": (\"6-9\", \"M\"), \"A00054766\": (\"6-9\", \"M\"), \"A00054836\": (\"6-9\", \"M\"), \"A00054917\": (\"6-9\", \"M\"),\n",
        "    \"A00055436\": (\"6-9\", \"M\"), \"A00055904\": (\"6-9\", \"M\"), \"A00055497\": (\"6-9\", \"M\"), \"A00054569\": (\"6-9\", \"M\"),\n",
        "    \"A00054666\": (\"6-9\", \"M\"), \"A00055473\": (\"6-9\", \"M\"), \"A00056002\": (\"6-9\", \"M\"), \"A00056257\": (\"6-9\", \"M\"),\n",
        "    \"A00056693\": (\"6-9\", \"M\"), \"A00054432\": (\"6-9\", \"F\"), \"A00055424\": (\"6-9\", \"F\"), \"A00055397\": (\"6-9\", \"F\"),\n",
        "    \"A00054597\": (\"6-9\", \"F\"), \"A00055526\": (\"6-9\", \"F\"), \"A00055628\": (\"6-9\", \"F\"), \"A00055547\": (\"6-9\", \"F\"),\n",
        "\n",
        "    \"A00051826\": (\"10-11\", \"M\"), \"A00053460\": (\"10-11\", \"M\"), \"A00054517\": (\"10-11\", \"M\"), \"A00054592\": (\"10-11\", \"M\"),\n",
        "    \"A00055392\": (\"10-11\", \"M\"), \"A00055429\": (\"10-11\", \"M\"), \"A00055613\": (\"10-11\", \"M\"), \"A00055623\": (\"10-11\", \"M\"),\n",
        "    \"A00055649\": (\"10-11\", \"M\"), \"A00056428\": (\"10-11\", \"M\"), \"A00058596\": (\"10-11\", \"M\"), \"A00055469\": (\"10-11\", \"M\"),\n",
        "    \"A00055486\": (\"10-11\", \"M\"), \"A00054535\": (\"10-11\", \"F\"), \"A00054748\": (\"10-11\", \"F\"), \"A00055293\": (\"10-11\", \"F\"),\n",
        "    \"A00054287\": (\"10-11\", \"F\"), \"A00055038\": (\"10-11\", \"F\"), \"A00055510\": (\"10-11\", \"F\"), \"A00055616\": (\"10-11\", \"F\"),\n",
        "\n",
        "    \"A00051886\": (\"12-13\", \"M\"), \"A00051955\": (\"12-13\", \"M\"), \"A00053398\": (\"12-13\", \"M\"), \"A00053440\": (\"12-13\", \"M\"),\n",
        "    \"A00053909\": (\"12-13\", \"M\"), \"A00054239\": (\"12-13\", \"M\"), \"A00054417\": (\"12-13\", \"M\"), \"A00055540\": (\"12-13\", \"M\"),\n",
        "    \"A00057135\": (\"12-13\", \"M\"), \"A00054907\": (\"12-13\", \"M\"), \"A00055662\": (\"12-13\", \"M\"), \"A00055682\": (\"12-13\", \"M\"),\n",
        "    \"A00055731\": (\"12-13\", \"M\"), \"A00055745\": (\"12-13\", \"M\"), \"A00055753\": (\"12-13\", \"M\"), \"A00059063\": (\"12-13\", \"M\"),\n",
        "    \"A00055801\": (\"12-13\", \"F\"), \"A00057599\": (\"12-13\", \"F\"), \"A00054359\": (\"12-13\", \"F\"), \"A00054469\": (\"12-13\", \"F\"),\n",
        "    \"A00054721\": (\"12-13\", \"F\"), \"A00054852\": (\"12-13\", \"F\"),\n",
        "\n",
        "    \"A00054817\": (\"14-17\", \"M\"), \"A00055055\": (\"14-17\", \"M\"), \"A00055077\": (\"14-17\", \"M\"), \"A00056054\": (\"14-17\", \"M\"),\n",
        "    \"A00056116\": (\"14-17\", \"M\"), \"A00055730\": (\"14-17\", \"M\"), \"A00055085\": (\"14-17\", \"M\"), \"A00055978\": (\"14-17\", \"M\"),\n",
        "    \"A00054647\": (\"14-17\", \"M\"), \"A00055482\": (\"14-17\", \"M\"), \"A00055502\": (\"14-17\", \"M\"), \"A00055853\": (\"14-17\", \"M\"),\n",
        "    \"A00056733\": (\"14-17\", \"M\"), \"A00054623\": (\"14-17\", \"M\"), \"A00055865\": (\"14-17\", \"M\"), \"A00056158\": (\"14-17\", \"M\"),\n",
        "    \"A00054369\": (\"14-17\", \"F\"), \"A00056723\": (\"14-17\", \"F\"), \"A00055103\": (\"14-17\", \"F\"), \"A00055837\": (\"14-17\", \"F\"),\n",
        "    \"A00055593\": (\"14-17\", \"F\"), \"A00056166\": (\"14-17\", \"F\"), \"A00056990\": (\"14-17\", \"F\"),\n",
        "\n",
        "    \"A00054930\": (\"18-24\", \"M\"), \"A00055065\": (\"18-24\", \"M\"), \"A00054039\": (\"18-24\", \"M\"), \"A00057092\": (\"18-24\", \"M\"),\n",
        "    \"A00062919\": (\"18-24\", \"M\"), \"A00058775\": (\"18-24\", \"M\"), \"A00053990\": (\"18-24\", \"M\"), \"A00054023\": (\"18-24\", \"M\"),\n",
        "    \"A00054387\": (\"18-24\", \"F\"), \"A00054207\": (\"18-24\", \"F\"), \"A00054122\": (\"18-24\", \"F\"), \"A00056640\": (\"18-24\", \"F\"),\n",
        "    \"A00059083\": (\"18-24\", \"F\"), \"A00063051\": (\"18-24\", \"F\"), \"A00056604\": (\"18-24\", \"F\"), \"A00063117\": (\"18-24\", \"F\"),\n",
        "\n",
        "    \"A00062219\": (\"25-44\", \"M\"), \"A00062408\": (\"25-44\", \"M\"), \"A00062125\": (\"25-44\", \"M\"), \"A00062578\": (\"25-44\", \"M\"),\n",
        "    \"A00062704\": (\"25-44\", \"M\"), \"A00062029\": (\"25-44\", \"M\"), \"A00062435\": (\"25-44\", \"M\"), \"A00062951\": (\"25-44\", \"M\"),\n",
        "    \"A00063029\": (\"25-44\", \"M\"), \"A00062279\": (\"25-44\", \"M\"), \"A00062842\": (\"25-44\", \"F\"), \"A00062453\": (\"25-44\", \"F\"),\n",
        "    \"A00062329\": (\"25-44\", \"F\"), \"A00063558\": (\"25-44\", \"F\"), \"A00062165\": (\"25-44\", \"F\"), \"A00063377\": (\"25-44\", \"F\"),\n",
        "    \"A00062055\": (\"25-44\", \"F\")\n",
        "}\n",
        "\n",
        "def add_demographics(df):\n",
        "    df['age_group'] = df['patient_id'].map(lambda x: patient_demographics.get(x, (None, None))[0])\n",
        "    df['gender'] = df['patient_id'].map(lambda x: patient_demographics.get(x, (None, None))[1])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11375c6-e85f-464f-8009-9c35473e6b63",
      "metadata": {
        "id": "d11375c6-e85f-464f-8009-9c35473e6b63"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# ? Helper: Evaluate by Group\n",
        "# ----------------------------\n",
        "def evaluate_by_group(y_true, y_pred, df, group_col):\n",
        "    y_pred = np.array(y_pred).flatten()\n",
        "    df_eval = pd.DataFrame({\n",
        "        \"true\": y_true,\n",
        "        \"pred\": (y_pred >= 0.5).astype(int),\n",
        "        group_col: df[group_col].values[:len(y_true)]\n",
        "    })\n",
        "    df_eval = df_eval.dropna(subset=[group_col])  # Drop rows where group_col is None\n",
        "\n",
        "    metric_frame = MetricFrame(\n",
        "        metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate,\n",
        "                 \"fpr\": false_positive_rate, \"tpr\": true_positive_rate},\n",
        "        y_true=df_eval[\"true\"],\n",
        "        y_pred=df_eval[\"pred\"],\n",
        "        sensitive_features=df_eval[group_col]\n",
        "    )\n",
        "    print(f\"\\n?? Fairness evaluation by {group_col}:\")\n",
        "    print(metric_frame.by_group)\n",
        "\n",
        "# ----------------------------\n",
        "# ? SHAP Fix\n",
        "# ----------------------------\n",
        "def explain_with_shap(model, loader, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    for batch in loader:\n",
        "        x_eeg, x_eye, x_beh, _ = batch\n",
        "        with torch.no_grad():\n",
        "            combined_input = torch.cat([x_eeg, x_eye, x_beh], dim=1)\n",
        "        features.append(combined_input.cpu().numpy())\n",
        "    combined_input = np.vstack(features)\n",
        "    explainer = shap.Explainer(model, combined_input)\n",
        "    shap_values = explainer(combined_input)\n",
        "    all_features = [f\"eeg_{i}\" for i in range(combined_input.shape[1])]\n",
        "    shap.summary_plot(shap_values.values, combined_input, feature_names=all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d2d0b2-ebee-4968-bd21-9b61273fa286",
      "metadata": {
        "id": "e0d2d0b2-ebee-4968-bd21-9b61273fa286"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# ? Fairness Threshold Postprocessing\n",
        "# ----------------------------\n",
        "def apply_threshold_optimizer(y_true, y_pred, sensitive_features):\n",
        "    preds = (np.array(y_pred) >= 0.5).astype(int)\n",
        "    to = ThresholdOptimizer(\n",
        "        estimator=None,\n",
        "        constraints=\"demographic_parity\",\n",
        "        prefit=True\n",
        "    )\n",
        "    to.fit(X=np.array(y_pred), y_true=np.array(y_true), sensitive_features=sensitive_features)\n",
        "    post_preds = to.predict(np.array(y_pred), sensitive_features=sensitive_features)\n",
        "\n",
        "    print(\"\\n? After ThresholdOptimizer:\")\n",
        "    print(classification_report(y_true, post_preds))\n",
        "\n",
        "# (Continue with training loop and main execution logic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492d4ea3-c660-460a-9e5f-c97bdcb04a7c",
      "metadata": {
        "id": "492d4ea3-c660-460a-9e5f-c97bdcb04a7c"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# ? Main Execution\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    balanced_df = add_demographics(balanced_df)\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    model = ImprovedTransformerFusion(\n",
        "        eeg_dim=len(EEG_FEATURES),\n",
        "        eye_dim=len(EYE_FEATURES),\n",
        "        beh_dim=len(BEHAV_FEATURES),\n",
        "        model_dim=32\n",
        "    ).to(device)\n",
        "\n",
        "    trained_model = train_model(model, train_loader, val_loader)\n",
        "\n",
        "    print(\"\\n?? Final Test Performance:\")\n",
        "    acc, y_true, y_pred = evaluate_model(trained_model, test_loader, device)\n",
        "\n",
        "    evaluate_by_group(y_true, y_pred, balanced_df, \"gender\")\n",
        "    evaluate_by_group(y_true, y_pred, balanced_df, \"age_group\")\n",
        "\n",
        "    print(\"\\n?? Post-processing Bias Mitigation...\")\n",
        "    apply_threshold_optimizer(y_true, y_pred, balanced_df[\"gender\"])\n",
        "\n",
        "    print(\"\\n?? Generating SHAP explainability report...\")\n",
        "    explain_with_shap(trained_model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74357b67-10af-434f-a65c-e574bc98df6b",
      "metadata": {
        "id": "74357b67-10af-434f-a65c-e574bc98df6b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98c4c68-59d9-490c-92e9-54d84631e99e",
      "metadata": {
        "id": "a98c4c68-59d9-490c-92e9-54d84631e99e"
      },
      "outputs": [],
      "source": [
        "# Refined SHAP Plotting for Edge Cases\n",
        "# Fixes dimensionality errors (like mismatched SHAP value shapes or failed plotting) and adds fallbacks:\n",
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def explain_with_shap(model, loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Collect a small batch for explanation\n",
        "    eeg_batch, eye_batch, beh_batch, _ = next(iter(loader))\n",
        "    eeg_sample = eeg_batch[:50].to(device)\n",
        "    eye_sample = eye_batch[:50].to(device)\n",
        "    beh_sample = beh_batch[:50].to(device)\n",
        "\n",
        "    def wrapped_model(X):\n",
        "        eeg_len = eeg_sample.shape[1]\n",
        "        eye_len = eye_sample.shape[1]\n",
        "        beh_len = beh_sample.shape[1]\n",
        "        eeg = torch.tensor(X[:, :eeg_len]).float().to(device)\n",
        "        eye = torch.tensor(X[:, eeg_len:eeg_len+eye_len]).float().to(device)\n",
        "        beh = torch.tensor(X[:, -beh_len:]).float().to(device)\n",
        "        return torch.sigmoid(model(eeg, eye, beh)).cpu().detach().numpy()\n",
        "\n",
        "    combined_input = torch.cat([eeg_sample.cpu(), eye_sample.cpu(), beh_sample.cpu()], dim=1).numpy()\n",
        "    explainer = shap.KernelExplainer(wrapped_model, combined_input)\n",
        "\n",
        "    try:\n",
        "        shap_values = explainer.shap_values(combined_input)\n",
        "        # Handle output as list or ndarray\n",
        "        if isinstance(shap_values, list) and len(shap_values) == 1:\n",
        "            shap_values = shap_values[0]\n",
        "        feature_names = [f\"eeg_{i}\" for i in range(eeg_sample.shape[1])] + \\\n",
        "                        [f\"eye_{i}\" for i in range(eye_sample.shape[1])] + \\\n",
        "                        [f\"beh_{i}\" for i in range(beh_sample.shape[1])]\n",
        "        shap.summary_plot(shap_values, combined_input, feature_names=feature_names)\n",
        "    except Exception as e:\n",
        "        print(\" SHAP plot failed:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1457b14-a56b-4a9a-996a-5b5e5e44fd47",
      "metadata": {
        "id": "c1457b14-a56b-4a9a-996a-5b5e5e44fd47"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a84a9e-7db1-432b-8122-8c79c511feea",
      "metadata": {
        "id": "05a84a9e-7db1-432b-8122-8c79c511feea"
      },
      "outputs": [],
      "source": [
        "#In-Processing Debiasing via ExponentiatedGradient (Fairlearn)\n",
        "#Train a fair classifier directly with demographic parity constraints:\n",
        "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def train_fairlearn_debiased(X_train, y_train, sensitive_features):\n",
        "    base_model = LogisticRegression(solver='liblinear')\n",
        "    mitigator = ExponentiatedGradient(\n",
        "        estimator=base_model,\n",
        "        constraints=DemographicParity(),\n",
        "        sample_weight_name='sample_weight'\n",
        "    )\n",
        "    mitigator.fit(X_train, y_train, sensitive_features=sensitive_features)\n",
        "    return mitigator\n",
        "#Extract Features and Labels from balanced_df\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Identify input features and labels\n",
        "feature_cols = [c for c in balanced_df.columns if c.startswith(\"eeg_\") or c.startswith(\"eye_\") or c.startswith(\"beh_\")]\n",
        "label_col = \"label\"\n",
        "sensitive_col = \"gender\"  # or \"age_group\"\n",
        "\n",
        "# Filter out rows missing demographic info\n",
        "fair_df = balanced_df.dropna(subset=[sensitive_col])\n",
        "\n",
        "# Prepare inputs\n",
        "X = fair_df[feature_cols].to_numpy()\n",
        "y = fair_df[label_col].to_numpy()\n",
        "sensitive_features = fair_df[sensitive_col].to_numpy()\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(\n",
        "    X, y, sensitive_features, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "#Train Fairlearn Debiased Model\n",
        "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the fair classifier\n",
        "base_estimator = LogisticRegression(solver='liblinear')\n",
        "mitigator = ExponentiatedGradient(\n",
        "    estimator=base_estimator,\n",
        "    constraints=DemographicParity(),\n",
        "    sample_weight_name=\"sample_weight\"\n",
        ")\n",
        "\n",
        "# Fit the fair model\n",
        "mitigator.fit(X_train, y_train, sensitive_features=s_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83491d4a-b187-4a3b-8878-81becba14b5c",
      "metadata": {
        "id": "83491d4a-b187-4a3b-8878-81becba14b5c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_fair = mitigator.predict(X_test)\n",
        "\n",
        "print(\" Debiased Fairlearn Model Accuracy:\", accuracy_score(y_test, y_pred_fair))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_fair))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c88537b-d2a9-4ea8-9177-433c7da79205",
      "metadata": {
        "id": "0c88537b-d2a9-4ea8-9177-433c7da79205"
      },
      "outputs": [],
      "source": [
        "###############################\n",
        "# 0.  BASIC SET-UP\n",
        "###############################\n",
        "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
        "from fairlearn.metrics import (\n",
        "    MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "###############################\n",
        "# 1.  DATA SPLIT + FEATURE MATRIX\n",
        "###############################\n",
        "def prepare_fairlearn_inputs(df: pd.DataFrame,\n",
        "                             sensitive_col: str = \"gender\",\n",
        "                             test_size: float = 0.3,\n",
        "                             random_state: int = 42):\n",
        "    feature_cols = [c for c in df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "    keep_df = df.dropna(subset=[sensitive_col])          # drop rows w/out demographic\n",
        "    X = keep_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "    y = keep_df[\"label\"].to_numpy().astype(\"int64\")\n",
        "    S = keep_df[sensitive_col].to_numpy()                # sensitive feature vector\n",
        "    return train_test_split(X, y, S,\n",
        "                            test_size=test_size,\n",
        "                            stratify=y,\n",
        "                            random_state=random_state)\n",
        "\n",
        "X_train, X_test, y_train, y_test, S_train, S_test = prepare_fairlearn_inputs(\n",
        "    balanced_df, sensitive_col=\"gender\"\n",
        ")\n",
        "\n",
        "###############################\n",
        "# 2.  HELPER: FAIRNESS METRICS\n",
        "###############################\n",
        "def fairness_report(y_true, y_pred, sensitive_features, title=\"\"):\n",
        "    \"\"\"Returns MetricFrame and prints group metrics.\"\"\"\n",
        "    frame = MetricFrame(\n",
        "        metrics={\n",
        "            \"accuracy\": accuracy_score,\n",
        "            \"selection_rate\": selection_rate,\n",
        "            \"fpr\": false_positive_rate,\n",
        "            \"tpr\": true_positive_rate\n",
        "        },\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        sensitive_features=sensitive_features\n",
        "    )\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(frame.by_group)\n",
        "    return frame\n",
        "\n",
        "\n",
        "###############################\n",
        "# 3.  BASELINE CLASSIFIER\n",
        "###############################\n",
        "baseline_clf = LogisticRegression(solver=\"liblinear\")\n",
        "baseline_clf.fit(X_train, y_train)\n",
        "y_pred_base = baseline_clf.predict(X_test)\n",
        "\n",
        "print(\"\\n*** BASELINE MODEL ***\")\n",
        "print(classification_report(y_test, y_pred_base, digits=4))\n",
        "mf_base = fairness_report(y_test, y_pred_base, S_test, title=\"Baseline Fairness\")\n",
        "\n",
        "\n",
        "###############################\n",
        "# 4.  DEBIASED CLASSIFIER (ExponentiatedGradient)\n",
        "###############################\n",
        "fair_clf = ExponentiatedGradient(\n",
        "    estimator=LogisticRegression(solver=\"liblinear\"),\n",
        "    constraints=DemographicParity()\n",
        ")\n",
        "fair_clf.fit(X_train, y_train, sensitive_features=S_train)\n",
        "y_pred_fair = fair_clf.predict(X_test)\n",
        "\n",
        "print(\"\\n*** DEBIASED MODEL (Fairlearn) ***\")\n",
        "print(classification_report(y_test, y_pred_fair, digits=4))\n",
        "mf_fair = fairness_report(y_test, y_pred_fair, S_test, title=\"Debiased Fairness\")\n",
        "\n",
        "\n",
        "###############################\n",
        "# 5.  PLOT GROUP-WISE METRICS\n",
        "###############################\n",
        "def barplot_metric(metric_name: str, mf_base: MetricFrame, mf_fair: MetricFrame,\n",
        "                   title_suffix: str = \"\"):\n",
        "    labels = mf_base.by_group.index.tolist()\n",
        "    base_vals = mf_base.by_group[metric_name].values\n",
        "    fair_vals = mf_fair.by_group[metric_name].values\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(x - width/2, base_vals, width=width, label=\"Baseline\")\n",
        "    plt.bar(x + width/2, fair_vals, width=width, label=\"Debiased\")\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title(f\"{metric_name} by group {title_suffix}\")\n",
        "    plt.xticks(x, labels, rotation=45)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for m in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    barplot_metric(m, mf_base, mf_fair, \"(gender)\")\n",
        "\n",
        "\n",
        "###############################\n",
        "# 6.  DROP-IN INTEGRATION\n",
        "###############################\n",
        "class FairlearnWrapper:\n",
        "    \"\"\"\n",
        "    Wraps a Fairlearn debiasing classifier so it can be swapped into\n",
        "    your existing deep feature extractor pipeline.\n",
        "\n",
        "    Usage:\n",
        "        1.  Extract embeddings/features from your deep model.\n",
        "        2.  Fit/predict via this wrapper.\n",
        "    \"\"\"\n",
        "    def __init__(self, constraint=DemographicParity()):\n",
        "        self.estimator = LogisticRegression(solver=\"liblinear\")\n",
        "        self.model = ExponentiatedGradient(\n",
        "            estimator=self.estimator,\n",
        "            constraints=constraint\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y, sensitive_features):\n",
        "        self.model.fit(X, y, sensitive_features=sensitive_features)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # ExponentiatedGradient exposes the underlying estimator’s predict_proba\n",
        "        return self.model._pmf_predict(X)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9833cbb-5464-48d2-905d-632f003bebef",
      "metadata": {
        "id": "e9833cbb-5464-48d2-905d-632f003bebef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46396da6-8a0e-400c-84c9-8dc0f6452af1",
      "metadata": {
        "scrolled": true,
        "id": "46396da6-8a0e-400c-84c9-8dc0f6452af1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import warnings\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ==== 1. Data Prep ====\n",
        "# Assume balanced_df already exists from upstream pipeline\n",
        "feature_cols = [c for c in balanced_df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "sensitive_col = \"gender\"\n",
        "label_col = \"label\"\n",
        "\n",
        "fair_df = balanced_df.dropna(subset=[sensitive_col])\n",
        "\n",
        "X = fair_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y = fair_df[label_col].to_numpy().astype(\"int64\")\n",
        "S = fair_df[sensitive_col].to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(\n",
        "    X, y, S, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ==== 2. Debiased Training ====\n",
        "strong_clf = HistGradientBoostingClassifier(max_iter=300, max_depth=5)\n",
        "fair_clf = ExponentiatedGradient(\n",
        "    estimator=strong_clf,\n",
        "    constraints=EqualizedOdds()\n",
        ")\n",
        "fair_clf.fit(X_train, y_train, sensitive_features=S_train)\n",
        "y_pred_fair = fair_clf.predict(X_test)\n",
        "\n",
        "print(\"\\n?? Classification Report (Debiased HGBC):\")\n",
        "print(classification_report(y_test, y_pred_fair, digits=4))\n",
        "\n",
        "# ==== 3. Fairness Report Function ====\n",
        "def fairness_report(y_true, y_pred, sensitive_features, title=\"\"):\n",
        "    frame = MetricFrame(\n",
        "        metrics={\"accuracy\": accuracy_score,\n",
        "                 \"selection_rate\": selection_rate,\n",
        "                 \"fpr\": false_positive_rate,\n",
        "                 \"tpr\": true_positive_rate},\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        sensitive_features=sensitive_features\n",
        "    )\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(frame.by_group)\n",
        "    return frame\n",
        "\n",
        "mf_fair = fairness_report(y_test, y_pred_fair, S_test, title=\"HGBC + EqualizedOdds (Gender)\")\n",
        "\n",
        "# ==== 4. Fairness Bar Plots ====\n",
        "def barplot_metric(metric_name: str, metric_frame: MetricFrame, title_suffix: str = \"\"):\n",
        "    labels = metric_frame.by_group.index.tolist()\n",
        "    values = metric_frame.by_group[metric_name].values\n",
        "    x = np.arange(len(labels))\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(x, values, width=0.5, color='steelblue')\n",
        "    plt.xticks(x, labels)\n",
        "    plt.title(f\"{metric_name} by {title_suffix}\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    barplot_metric(metric, mf_fair, title_suffix=sensitive_col)\n",
        "\n",
        "# ==== 5. SHAP Wrapper for ExponentiatedGradient ====\n",
        "def fairlearn_shap_explainer(fair_clf, X_sample):\n",
        "    \"\"\"Compute SHAP for Fairlearn ensemble via weighted base models.\"\"\"\n",
        "    shap_values_ensemble = np.zeros_like(X_sample)\n",
        "    for predictor, weight in zip(fair_clf.predictors_, fair_clf.weights_):\n",
        "        explainer = shap.Explainer(predictor.predict, X_sample)\n",
        "        shap_values = explainer(X_sample).values\n",
        "        shap_values_ensemble += weight * shap_values\n",
        "    return shap_values_ensemble\n",
        "\n",
        "X_sample = X_train[:50]  # avoid OOM\n",
        "shap_vals = fairlearn_shap_explainer(fair_clf, X_sample)\n",
        "shap.summary_plot(shap_vals, X_sample, feature_names=feature_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99411c44-7438-44bc-ad58-092349148e79",
      "metadata": {
        "id": "99411c44-7438-44bc-ad58-092349148e79"
      },
      "outputs": [],
      "source": [
        "# ? Full Age-Based Fairness Evaluation Pipeline (HGBC + EqualizedOdds)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === 1. Feature Preparation ===\n",
        "feature_cols = [c for c in balanced_df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "sensitive_col = \"age_group\"  # <- use already binned age groups\n",
        "label_col = \"label\"\n",
        "\n",
        "# === 2. Filter rows with valid age_group ===\n",
        "fair_df = balanced_df.dropna(subset=[sensitive_col])\n",
        "\n",
        "# === 3. Split data ===\n",
        "X = fair_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y = fair_df[label_col].to_numpy().astype(\"int64\")\n",
        "S = fair_df[sensitive_col].astype(str).to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(\n",
        "    X, y, S, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# === 4. Train Debiased Model ===\n",
        "strong_clf = HistGradientBoostingClassifier(max_iter=300, max_depth=5)\n",
        "fair_clf = ExponentiatedGradient(estimator=strong_clf, constraints=EqualizedOdds())\n",
        "fair_clf.fit(X_train, y_train, sensitive_features=S_train)\n",
        "y_pred_fair = fair_clf.predict(X_test)\n",
        "\n",
        "# === 5. Report Performance ===\n",
        "print(\"\\n?? Classification Report (Debiased HGBC - Age Groups):\")\n",
        "print(classification_report(y_test, y_pred_fair, digits=4))\n",
        "\n",
        "# === 6. Fairness Evaluation ===\n",
        "def fairness_report(y_true, y_pred, sensitive_features, title=\"\"):\n",
        "    frame = MetricFrame(\n",
        "        metrics={\n",
        "            \"accuracy\": accuracy_score,\n",
        "            \"selection_rate\": selection_rate,\n",
        "            \"fpr\": false_positive_rate,\n",
        "            \"tpr\": true_positive_rate\n",
        "        },\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        sensitive_features=sensitive_features\n",
        "    )\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(frame.by_group)\n",
        "    return frame\n",
        "\n",
        "mf_fair_age = fairness_report(y_test, y_pred_fair, S_test, title=\"HGBC + EqualizedOdds (Age Group)\")\n",
        "\n",
        "# === 7. Visualization ===\n",
        "def barplot_metric(metric_name: str, metric_frame: MetricFrame, title_suffix: str = \"\"):\n",
        "    labels = metric_frame.by_group.index.tolist()\n",
        "    values = metric_frame.by_group[metric_name].values\n",
        "    x = np.arange(len(labels))\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(x, values, width=0.5, color='darkcyan')\n",
        "    plt.xticks(x, labels, rotation=45)\n",
        "    plt.title(f\"{metric_name} by {title_suffix}\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    barplot_metric(metric, mf_fair_age, title_suffix=\"age_group\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334858cb-5c7c-4cb9-bd9e-74b492f9df58",
      "metadata": {
        "id": "334858cb-5c7c-4cb9-bd9e-74b492f9df58"
      },
      "outputs": [],
      "source": [
        "# 🚀 Enhanced Fairness‑Aware Training Pipeline\n",
        "# Updated: Replaces GridSearch with ExponentiatedGradient (stable)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.utils import resample\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        "import shap\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === 0. Config ===\n",
        "RANDOM_STATE = 42\n",
        "RESAMPLE_N   = 120\n",
        "\n",
        "# === 1. Feature Prep ===\n",
        "feature_cols = [c for c in balanced_df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "label_col    = \"label\"\n",
        "sensitive_col = \"age_group\"\n",
        "\n",
        "fair_df = balanced_df.dropna(subset=[sensitive_col])\n",
        "\n",
        "# === 2. Resample to balance age groups ===\n",
        "resampled = []\n",
        "for grp in fair_df[sensitive_col].unique():\n",
        "    grp_df = fair_df[fair_df[sensitive_col] == grp]\n",
        "    resampled.append(resample(grp_df, replace=True, n_samples=RESAMPLE_N, random_state=RANDOM_STATE))\n",
        "\n",
        "balanced_fair_df = pd.concat(resampled)\n",
        "\n",
        "# === 3. Split ===\n",
        "X = balanced_fair_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y = balanced_fair_df[label_col].to_numpy().astype(\"int64\")\n",
        "S = balanced_fair_df[sensitive_col].astype(str).to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(\n",
        "    X, y, S, test_size=0.25, stratify=y, random_state=RANDOM_STATE)\n",
        "\n",
        "# === 4. Tune Base HGBC ===\n",
        "param_grid = {\n",
        "    \"max_iter\": [300, 500],\n",
        "    \"max_depth\": [3, 5],\n",
        "    \"learning_rate\": [0.01, 0.05]\n",
        "}\n",
        "base_hgbc = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "cv_hgbc  = GridSearchCV(base_hgbc, param_grid, cv=3, n_jobs=-1)\n",
        "cv_hgbc.fit(X_train, y_train)\n",
        "print(\"Best HGBC params:\", cv_hgbc.best_params_)\n",
        "\n",
        "# === 5. Debias using ExponentiatedGradient ===\n",
        "fair_clf = ExponentiatedGradient(\n",
        "    estimator=cv_hgbc.best_estimator_,\n",
        "    constraints=EqualizedOdds(),\n",
        "    sample_weight_name=\"sample_weight\"\n",
        ")\n",
        "fair_clf.fit(X_train, y_train, sensitive_features=S_train)\n",
        "\n",
        "# === 6. Evaluation ===\n",
        "y_pred = fair_clf.predict(X_test)\n",
        "print(\"\\n📋 Classification Report (Debiased HGBC tuned):\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC‑AUC:\", roc_auc_score(y_test, fair_clf._pmf_predict(X_test)[:,1]))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "mf_age = MetricFrame(\n",
        "    metrics={\n",
        "        \"accuracy\": accuracy_score,\n",
        "        \"selection_rate\": selection_rate,\n",
        "        \"fpr\": false_positive_rate,\n",
        "        \"tpr\": true_positive_rate\n",
        "    },\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    sensitive_features=S_test\n",
        ")\n",
        "print(\"\\n=== Fairness (Age Group) ===\")\n",
        "print(mf_age.by_group)\n",
        "\n",
        "# === 7. Plot Metrics ===\n",
        "for m in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    labels = mf_age.by_group.index.tolist()\n",
        "    vals   = mf_age.by_group[m].values\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(range(len(labels)), vals, color=\"teal\")\n",
        "    plt.xticks(range(len(labels)), labels, rotation=45)\n",
        "    plt.title(f\"{m} by age_group\")\n",
        "    plt.ylim(0,1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === 8. SHAP Explainability ===\n",
        "explainer = shap.Explainer(fair_clf.predict, X_train[:50])\n",
        "shap_vals = explainer(X_test[:50])\n",
        "shap.summary_plot(shap_vals.values, X_test[:50], feature_names=feature_cols)\n",
        "\n",
        "# === 9. Save Model ===\n",
        "joblib.dump(fair_clf, \"debiased_hgbc_exponentiated.pkl\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0890893f-f789-4ae0-9e07-e6cb43bf3e3e",
      "metadata": {
        "id": "0890893f-f789-4ae0-9e07-e6cb43bf3e3e"
      },
      "outputs": [],
      "source": [
        "# 🚀 Enhanced Fairness‑Aware Training Pipeline (with Intersectional Fairness)\n",
        "# Updated: Replaces GridSearch with ExponentiatedGradient (stable)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.utils import resample\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        "import shap\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === 0. Config ===\n",
        "RANDOM_STATE = 42\n",
        "RESAMPLE_N   = 120\n",
        "\n",
        "# === 1. Feature Prep ===\n",
        "feature_cols = [c for c in balanced_df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "label_col    = \"label\"\n",
        "sensitive_col1 = \"age_group\"\n",
        "sensitive_col2 = \"gender\"\n",
        "\n",
        "# Create intersectional fairness column\n",
        "balanced_df[\"intersection\"] = balanced_df[sensitive_col2].astype(str) + \"_\" + balanced_df[sensitive_col1].astype(str)\n",
        "sensitive_col = \"intersection\"\n",
        "\n",
        "fair_df = balanced_df.dropna(subset=[sensitive_col])\n",
        "\n",
        "# === 2. Resample to balance intersectional groups ===\n",
        "resampled = []\n",
        "for grp in fair_df[sensitive_col].unique():\n",
        "    grp_df = fair_df[fair_df[sensitive_col] == grp]\n",
        "    resampled.append(resample(grp_df, replace=True, n_samples=RESAMPLE_N, random_state=RANDOM_STATE))\n",
        "\n",
        "balanced_fair_df = pd.concat(resampled)\n",
        "\n",
        "# === 3. Split ===\n",
        "X = balanced_fair_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y = balanced_fair_df[label_col].to_numpy().astype(\"int64\")\n",
        "S = balanced_fair_df[sensitive_col].astype(str).to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(\n",
        "    X, y, S, test_size=0.25, stratify=y, random_state=RANDOM_STATE)\n",
        "\n",
        "# === 4. Tune Base HGBC ===\n",
        "param_grid = {\n",
        "    \"max_iter\": [300, 500],\n",
        "    \"max_depth\": [3, 5],\n",
        "    \"learning_rate\": [0.01, 0.05]\n",
        "}\n",
        "base_hgbc = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "cv_hgbc  = GridSearchCV(base_hgbc, param_grid, cv=3, n_jobs=-1)\n",
        "cv_hgbc.fit(X_train, y_train)\n",
        "print(\"Best HGBC params:\", cv_hgbc.best_params_)\n",
        "\n",
        "# === 5. Debias using ExponentiatedGradient ===\n",
        "fair_clf = ExponentiatedGradient(\n",
        "    estimator=cv_hgbc.best_estimator_,\n",
        "    constraints=EqualizedOdds(),\n",
        "    sample_weight_name=\"sample_weight\"\n",
        ")\n",
        "fair_clf.fit(X_train, y_train, sensitive_features=S_train)\n",
        "\n",
        "# === 6. Evaluation ===\n",
        "y_pred = fair_clf.predict(X_test)\n",
        "print(\"\\n📋 Classification Report (Debiased HGBC tuned):\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC‑AUC:\", roc_auc_score(y_test, fair_clf._pmf_predict(X_test)[:,1]))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "mf_intersection = MetricFrame(\n",
        "    metrics={\n",
        "        \"accuracy\": accuracy_score,\n",
        "        \"selection_rate\": selection_rate,\n",
        "        \"fpr\": false_positive_rate,\n",
        "        \"tpr\": true_positive_rate\n",
        "    },\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    sensitive_features=S_test\n",
        ")\n",
        "print(\"\\n=== Fairness (Intersectional: Gender × Age Group) ===\")\n",
        "print(mf_intersection.by_group)\n",
        "\n",
        "# === 7. Plot Metrics ===\n",
        "for m in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    labels = mf_intersection.by_group.index.tolist()\n",
        "    vals   = mf_intersection.by_group[m].values\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.bar(range(len(labels)), vals, color=\"darkorange\")\n",
        "    plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
        "    plt.title(f\"{m} by intersectional group\")\n",
        "    plt.ylim(0,1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === 8. SHAP Explainability ===\n",
        "explainer = shap.Explainer(fair_clf.predict, X_train[:50])\n",
        "shap_vals = explainer(X_test[:50])\n",
        "shap.summary_plot(shap_vals.values, X_test[:50], feature_names=feature_cols)\n",
        "\n",
        "# === 8b. Intersectional SHAP ===\n",
        "intersection_groups = pd.Series(S_test).unique()\n",
        "for group in intersection_groups:\n",
        "    idx = np.where(S_test == group)[0][:10]  # small sample per group\n",
        "    if len(idx) == 0: continue\n",
        "    group_shap_vals = explainer(X_test[idx])\n",
        "    print(f\"\\n💡 SHAP Summary for Group: {group}\")\n",
        "    shap.summary_plot(group_shap_vals.values, X_test[idx], feature_names=feature_cols, show=True)\n",
        "\n",
        "# === 8c. Automated Fairness Violation Detection ===\n",
        "violations = []\n",
        "threshold = 0.2  # configurable disparity threshold\n",
        "for metric in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    disparity = mf_intersection.by_group[metric].max() - mf_intersection.by_group[metric].min()\n",
        "    if disparity > threshold:\n",
        "        violations.append((metric, round(disparity, 3)))\n",
        "\n",
        "if violations:\n",
        "    print(\"\\n⚠️ Fairness Violations Detected:\")\n",
        "    for metric, gap in violations:\n",
        "        print(f\"- {metric} disparity = {gap} exceeds threshold {threshold}\")\n",
        "else:\n",
        "    print(\"\\n✅ No fairness violations detected (within threshold).\")\n",
        "\n",
        "# === 9. Save Model ===\n",
        "joblib.dump(fair_clf, \"debiased_hgbc_intersectional.pkl\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f586d974-11ff-4c7c-b1fd-4d379db3402c",
      "metadata": {
        "id": "f586d974-11ff-4c7c-b1fd-4d379db3402c"
      },
      "outputs": [],
      "source": [
        "!pip install dowhy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73af0f91-dba8-4cc1-ba6a-491c8aac31e2",
      "metadata": {
        "id": "73af0f91-dba8-4cc1-ba6a-491c8aac31e2"
      },
      "outputs": [],
      "source": [
        "!pip install econml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3244709f-ed3a-40d1-a89e-9c603124e0cf",
      "metadata": {
        "id": "3244709f-ed3a-40d1-a89e-9c603124e0cf"
      },
      "outputs": [],
      "source": [
        "!pip install aequitas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ba8a13-bdb1-4920-99a0-3cbfdb4daf63",
      "metadata": {
        "id": "b2ba8a13-bdb1-4920-99a0-3cbfdb4daf63"
      },
      "outputs": [],
      "source": [
        "!pip install interpret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59408d3f-84ec-454c-8f53-82cdd6e440d5",
      "metadata": {
        "id": "59408d3f-84ec-454c-8f53-82cdd6e440d5"
      },
      "outputs": [],
      "source": [
        "# 🚀 Enhanced Fairness​-Aware Training Pipeline (Extended)\n",
        "# Updated: Fix DoWhy binary treatment error for causal fairness + final improvements\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.utils import resample\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
        "from fairlearn.postprocessing import ThresholdOptimizer\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        "import shap\n",
        "import joblib\n",
        "from collections import Counter\n",
        "from dowhy import CausalModel\n",
        "from econml.dr import DRLearner\n",
        "from aequitas.group import Group\n",
        "from aequitas.preprocessing import preprocess_input_df\n",
        "from aequitas.bias import Bias\n",
        "from aequitas.fairness import Fairness\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "from interpret import show\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === 0. Config ===\n",
        "RANDOM_STATE = 42\n",
        "RESAMPLE_N   = 120\n",
        "\n",
        "# === 1. Feature Prep ===\n",
        "feature_cols = [c for c in balanced_df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "label_col    = \"label\"\n",
        "sensitive_col1 = \"age_group\"\n",
        "sensitive_col2 = \"gender\"\n",
        "\n",
        "balanced_df[\"intersection\"] = balanced_df[sensitive_col2].astype(str) + \"_\" + balanced_df[sensitive_col1].astype(str)\n",
        "sensitive_col = \"intersection\"\n",
        "\n",
        "fair_df = balanced_df.dropna(subset=[sensitive_col])\n",
        "\n",
        "# === 2. Resample to balance intersectional groups ===\n",
        "resampled = []\n",
        "for grp in fair_df[sensitive_col].unique():\n",
        "    grp_df = fair_df[fair_df[sensitive_col] == grp]\n",
        "    resampled.append(resample(grp_df, replace=True, n_samples=RESAMPLE_N, random_state=RANDOM_STATE))\n",
        "\n",
        "balanced_fair_df = pd.concat(resampled)\n",
        "\n",
        "# === 3. Split ===\n",
        "X = balanced_fair_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y = balanced_fair_df[label_col].to_numpy().astype(\"int64\")\n",
        "S = balanced_fair_df[sensitive_col].astype(str).to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(\n",
        "    X, y, S, test_size=0.25, stratify=y, random_state=RANDOM_STATE)\n",
        "\n",
        "# === 4. Tune Base HGBC ===\n",
        "param_grid = {\n",
        "    \"max_iter\": [300, 500],\n",
        "    \"max_depth\": [3, 5],\n",
        "    \"learning_rate\": [0.01, 0.05]\n",
        "}\n",
        "base_hgbc = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "cv_hgbc  = GridSearchCV(base_hgbc, param_grid, cv=3, n_jobs=1)\n",
        "cv_hgbc.fit(X_train, y_train)\n",
        "print(\"Best HGBC params:\", cv_hgbc.best_params_)\n",
        "\n",
        "# === 5. Debiased HGBC ===\n",
        "fair_clf = ExponentiatedGradient(estimator=cv_hgbc.best_estimator_, constraints=EqualizedOdds(), sample_weight_name=\"sample_weight\")\n",
        "fair_clf.fit(X_train, y_train, sensitive_features=S_train)\n",
        "y_pred = fair_clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, fair_clf._pmf_predict(X_test)[:,1]))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# === 6. Threshold Optimizer (skip degenerate groups) ===\n",
        "def filter_degenerate(X, y, S):\n",
        "    df = pd.DataFrame(X)\n",
        "    df['label'] = y\n",
        "    df['sensitive'] = S\n",
        "    valid_groups = []\n",
        "    for g, group_df in df.groupby('sensitive'):\n",
        "        label_counts = Counter(group_df['label'])\n",
        "        if len(label_counts) > 1:\n",
        "            valid_groups.append(g)\n",
        "    idx = [i for i, s in enumerate(S) if s in valid_groups]\n",
        "    print(\"Excluded degenerate groups:\", set(S) - set(valid_groups))\n",
        "    return X[idx], y[idx], S[idx]\n",
        "\n",
        "X_train_fd, y_train_fd, S_train_fd = filter_degenerate(X_train, y_train, S_train)\n",
        "X_test_fd, y_test_fd, S_test_fd = filter_degenerate(X_test, y_test, S_test)\n",
        "\n",
        "thresh = ThresholdOptimizer(estimator=cv_hgbc.best_estimator_, constraints=\"equalized_odds\", prefit=True)\n",
        "thresh.fit(X_train_fd, y_train_fd, sensitive_features=S_train_fd)\n",
        "y_thresh_pred = thresh.predict(X_test_fd, sensitive_features=S_test_fd)\n",
        "\n",
        "# === 7. Fairness MetricFrame ===\n",
        "mf_intersection = MetricFrame(metrics={\"accuracy\": accuracy_score,\"selection_rate\": selection_rate,\"fpr\": false_positive_rate,\"tpr\": true_positive_rate}, y_true=y_test, y_pred=y_pred, sensitive_features=S_test)\n",
        "print(\"\\n=== Fairness (Intersectional: Gender × Age Group) ===\")\n",
        "print(mf_intersection.by_group)\n",
        "\n",
        "# === Export fairness report ===\n",
        "mf_intersection.by_group.to_csv(\"fairness_intersectional_report.csv\")\n",
        "\n",
        "# === 8. Plot ===\n",
        "for m in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    labels = mf_intersection.by_group.index.tolist()\n",
        "    vals   = mf_intersection.by_group[m].values\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.bar(range(len(labels)), vals, color=\"darkorange\")\n",
        "    plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
        "    plt.title(f\"{m} by intersectional group\")\n",
        "    plt.ylim(0,1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === 9. SHAP + Interaction ===\n",
        "explainer = shap.Explainer(fair_clf.predict, X_train[:50])\n",
        "shap_vals = explainer(X_test[:50])\n",
        "shap.summary_plot(shap_vals.values, X_test[:50], feature_names=feature_cols)\n",
        "interaction_vals = shap.TreeExplainer(cv_hgbc.best_estimator_).shap_interaction_values(X_test[:50])\n",
        "feature_names_arr = np.array(feature_cols)\n",
        "shap.summary_plot(np.array(interaction_vals), X_test[:50], feature_names=feature_names_arr)\n",
        "\n",
        "# === 10. Counterfactual ===\n",
        "def simulate_counterfactual(X, S, old_value, new_value):\n",
        "    S_cf = np.array([new_value if s == old_value else s for s in S])\n",
        "    delta = np.mean(fair_clf.predict(X) != fair_clf.predict(X))\n",
        "    print(f\"Counterfactual prediction flip rate ({old_value} ➔ {new_value}):\", round(delta, 4))\n",
        "simulate_counterfactual(X_test, S_test, \"F_18-24\", \"M_25-44\")\n",
        "\n",
        "# === 11. Violations ===\n",
        "violations = []\n",
        "for metric in [\"selection_rate\", \"fpr\", \"tpr\"]:\n",
        "    disparity = mf_intersection.by_group[metric].max() - mf_intersection.by_group[metric].min()\n",
        "    if disparity > 0.2:\n",
        "        violations.append((metric, round(disparity, 3)))\n",
        "if violations:\n",
        "    print(\"\\n⚠️ Fairness Violations Detected:\")\n",
        "    for metric, gap in violations:\n",
        "        print(f\"- {metric} disparity = {gap} exceeds threshold 0.2\")\n",
        "else:\n",
        "    print(\"\\n✅ No fairness violations detected (within threshold).\")\n",
        "\n",
        "# === 12. Causal Fairness (Fixed to binary treatment) ===\n",
        "causal_df = balanced_fair_df.copy()\n",
        "causal_df['treatment_binary'] = (causal_df['intersection'] == \"F_18-24\").astype(int)\n",
        "model = CausalModel(data=causal_df, treatment=\"treatment_binary\", outcome=label_col, common_causes=feature_cols[:3])\n",
        "model.view_model()\n",
        "identified_estimand = model.identify_effect()\n",
        "estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.propensity_score_matching\", method_params={\"fit_params\": {}})\n",
        "print(\"\\n🌟 Causal effect estimate (F_18-24 vs others):\", estimate.value)\n",
        "\n",
        "# === 13. EconML Uplift (Fix regression requirement) ===\n",
        "uplift_model = DRLearner(model_regression=HistGradientBoostingRegressor(), model_final=HistGradientBoostingRegressor(), discrete_outcome=False)\n",
        "uplift_model.fit(Y=y_train, T=(S_train == \"F_18-24\").astype(int), X=X_train)\n",
        "uplift_te = uplift_model.effect(X_test)\n",
        "print(\"\\n📊 EconML Uplift Estimates (sample):\", uplift_te[:5])\n",
        "\n",
        "# === 14. Aequitas Dashboard (Final fix for tuple unpack) ===\n",
        "df_aeq = pd.DataFrame({\"score\": y_pred, \"label_value\": y_test, \"attribute_name\": S_test})\n",
        "preprocessed = preprocess_input_df(df_aeq)\n",
        "input_df = preprocessed[0] if isinstance(preprocessed, tuple) else preprocessed\n",
        "\n",
        "if isinstance(input_df, tuple):\n",
        "    input_df = input_df[0]\n",
        "\n",
        "g = Group()\n",
        "gdf = g.get_crosstabs(input_df)\n",
        "b = Bias()\n",
        "bdf = b.get_disparity_predefined_groups(gdf, original_df=input_df.copy(), ref_groups_dict={\"attribute_name\": \"F_18-24\"}, alpha=0.05)\n",
        "print(\"\\n📊 Aequitas Bias Summary:\")\n",
        "print(bdf[[\"attribute_name\", \"label_value\", \"ppr_disparity\", \"stat_par\"]].head())\n",
        "f = Fairness()\n",
        "fdf = f.get_group_value_fairness(bdf)\n",
        "print(\"\\n📈 Fairness Summary:\")\n",
        "print(fdf.head())\n",
        "\n",
        "# === 15. Explainable Boosting Machine (EBM) ===\n",
        "ebm = ExplainableBoostingClassifier(random_state=RANDOM_STATE)\n",
        "ebm.fit(X_train, y_train)\n",
        "print(\"\\n🔍 EBM Accuracy:\", accuracy_score(y_test, ebm.predict(X_test)))\n",
        "show(ebm.explain_global())\n",
        "\n",
        "# === 16. Save Model ===\n",
        "joblib.dump(fair_clf, \"debiased_hgbc_intersectional.pkl\")\n",
        "joblib.dump(ebm, \"interpretable_ebm_model.pkl\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdfb96c-3c26-401a-a16b-ec8118f50c23",
      "metadata": {
        "id": "7cdfb96c-3c26-401a-a16b-ec8118f50c23"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}